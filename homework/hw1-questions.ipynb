{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW will help you review some ideas of linear algebra, revising basic identities of matrix calculus in part 1 (you can prove most of the identities by writing out the components or by using other identities), and eigenvector decomposition in Part 2. Finally in part 3 you will work through a polynomial regression problem similar to the one we discussed in class this week. The HW is made of many short questions, and I also provide several definitions and hints throughout that will help you.\n",
    "\n",
    "Please answer all the questions involving proofs/demonstrations using pen and paper, and all the numerical excercises using an ipython notebook.\n",
    "\n",
    "You can also view this ipython notebook in your browser through nbviewer by following this [link](https://nbviewer.jupyter.org/github/smcantab/chen5595-fall2020/blob/master/homework/hw1-questions.ipynb). If you want to be able to modify and execute this script in the cloud you can launch it in Binder either by clicking \"Execute on Binder\" on the top right of the page in nbviewer, or by following this [link](https://mybinder.org/v2/gh/smcantab/chen5595-fall2020/d8e260074b28d8fda3aa30a4ca4acf8fba84ee15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Matrix calculus\n",
    "\n",
    "Let $a_{ij} \\in \\mathbb{R}$, $i=1,2,\\dots, m$, $j=1, 2, \\dots, n$, we define $\\mathbf{A}$ to be the _real matrix_ of dimensions $m \\times n$:\n",
    "\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{pmatrix}$$\n",
    "    \n",
    "Notation: $\\mathbf{A}^\\top$ denotes the matrix transpose; $\\mathbf{A}^{-1}$ denotes the inverse matrix; $|\\mathbf{A}|$ denotes tha matrix determinant; $\\mathrm{Tr}(\\mathbf{A})$ denotes the trace of $\\mathbf{A}$. The identity matrix is $\\mathbf{I}$ and the null matrix is $\\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "\n",
    "The matrix multiplication is defined as follows: let $\\mathbf{A}$ be an $m \\times n$ matrix and $\\mathbf{B}$ and $n \\times p$ matrix, then their product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ is an $m \\times p$ matrix with element (i, j) given by  \n",
    "\n",
    "$$c_{ij} = \\sum_{k=1}^n a_{ik} b_{kj}$$\n",
    "    \n",
    "for all $i=1,2,\\dots, m$, $j=1, 2, \\dots, p$.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioned matrix\n",
    "\n",
    "Let $\\mathbf{A}$ be a $m \\times n$ matrix, its _partition_ is\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix} \\mathbf{B} & \\mathbf{C} \\\\ \\mathbf{D} & \\mathbf{E} \\end{pmatrix}$$\n",
    "    \n",
    "where $\\mathbf{B}$ is $m_1 \\times n_1$, $\\mathbf{C}$ is $m_1 \\times n_2$, $\\mathbf{D}$ is $m_2 \\times n_1$, $\\mathbf{E}$ is $m_2 \\times n_2$, such that $m_1 + m_2 = m$ and $n_1 + n_2 = n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix differentiation\n",
    "\n",
    "Let $\\mathbf{y} = f(\\mathbf{x})$ where $\\mathbf{y}$ is an $m$-element vector and $\\mathbf{x}$ an $n$-element vector. The Jacobian matrix of the transformation $f(\\mathbf{x})$ is\n",
    "\n",
    "$$\\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\dfrac{\\partial y_1}{\\partial x_1} & \\dfrac{\\partial y_1}{\\partial x_2} & \\dots & \\dfrac{\\partial y_1}{\\partial x_n} \\\\ \\dfrac{\\partial y_2}{\\partial x_1} & \\dfrac{\\partial y_2}{\\partial x_2} & \\dots & \\dfrac{\\partial y_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\dfrac{\\partial y_m}{\\partial x_1} & \\dfrac{\\partial y_m}{\\partial x_2} & \\dots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix}$$\n",
    "\n",
    "Notice that if $\\mathbf{x}$ is a scalar, the resulting Jacobian matrix is an $m \\times 1$ matrix. If $\\mathbf{y}$ is a scalar, the resulting Jacobian is a $1 \\times n$ matrix.\n",
    "    \n",
    "Let $\\mathbf{A}$ be an $m \\times n$ matric whose elements are functions of the scalar parameter $q$. Then the derivative of $\\mathbf{A}$ with respect to $q$ is the element wise derivative of $\\mathbf{A}$ with respect to $q$:\n",
    "    \n",
    "$$\\dfrac{\\partial \\mathbf{A}}{\\partial q} = \\begin{pmatrix} \\dfrac{\\partial a_{11}}{\\partial q} & \\dfrac{\\partial a_{12}}{\\partial q} & \\dots & \\dfrac{\\partial a_{1n}}{\\partial q} \\\\ \\dfrac{\\partial a_{21}}{\\partial q} & \\dfrac{\\partial a_{22}}{\\partial q} & \\dots & \\dfrac{\\partial a_{2n}}{\\partial q} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\dfrac{\\partial a_{m1}}{\\partial q} & \\dfrac{\\partial a_{m2}}{\\partial q} & \\dots & \\dfrac{\\partial a_{mn}}{\\partial q} \\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "Let $\\mathbf{A}$ be an $n \\times m$ matrix and $\\mathbf{B}$ and $m \\times p$ matrix, and let their product be $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$. Prove that (_hint: compare the entries of $(\\mathbf{A}\\mathbf{B})^\\top$ and $\\mathbf{B}^\\top \\mathbf{A}^\\top$_)\n",
    "\n",
    "$$\\mathbf{C}^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Let $\\mathbf{A}$ and $\\mathbf{B}$ be $n \\times n$ invertible matrices, and let their product be $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$. Prove that \n",
    "\n",
    "$$\\mathbf{C}^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "\n",
    "Let $\\mathbf{A}$ be an $m \\times m$ square matrix. Partition $\\mathbf{A}$ as\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} & \\mathbf{A}_{22} \\end{pmatrix}$$\n",
    "    \n",
    "so that $\\mathbf{A}_{11}$ is a nonsingular $m_1 \\times m_1$ matrix, $\\mathbf{A}_{22}$ is a nonsingular $m_2 \\times m_2$ matrix, so that $m_1 + m_2 = m$. Prove (_hint: by direct multiplication_) that\n",
    "    \n",
    "$$\\mathbf{A}^{-1} = \\begin{pmatrix} (\\mathbf{A}_{11}-\\mathbf{A}_{12}\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21})^{-1} & -\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12}(\\mathbf{A}_{22}-\\mathbf{A}_{21}\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12})^{-1} \\\\ -\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21}(\\mathbf{A}_{11}-\\mathbf{A}_{12}\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21})^{-1} & (\\mathbf{A}_{22}-\\mathbf{A}_{21}\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12})^{-1} \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4\n",
    "Let\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{A} \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{y}$ is $m \\times 1$, $\\mathbf{x}$ is $n \\times 1$, $\\mathbf{A}$ is $m \\times n$, and $\\mathbf{A}$ does not depend on $\\mathbf{x}$. Prove that  \n",
    "\n",
    "$$\\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\mathbf{A}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5\n",
    "\n",
    "Let\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{A} \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{y}$ is $m \\times 1$, $\\mathbf{x}$ is $n \\times 1$, $\\mathbf{A}$ is $m \\times n$, and $\\mathbf{A}$ does not depend on $\\mathbf{x}$. Suppose that $\\mathbf{x}$ is a function of the vector $\\mathbf{z}$, while $\\mathbf{A}$ is independent of $\\mathbf{z}.$ Prove that  \n",
    "\n",
    "$$\\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{z}} = \\mathbf{A}\\dfrac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.6\n",
    "\n",
    "Let the scalar $q$ be defined by\n",
    "$$q = \\mathbf{y}^\\top \\mathbf{A} \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{y}$ is $m \\times 1$, $\\mathbf{x}$ is $n \\times 1$, $\\mathbf{A}$ is $m \\times n$, and $\\mathbf{A}$ does not depend on $\\mathbf{x}$ or $\\mathbf{y}$. Prove that (_hint: notice that we can write $\\mathbf{w}^\\top = \\mathbf{y}^\\top \\mathbf{A}$ and that $q^T = q$_) \n",
    "\n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{x}} = \\mathbf{y}^\\top \\mathbf{A}$$\n",
    "    \n",
    "and\n",
    "    \n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{y}} = \\mathbf{x}^\\top \\mathbf{A}^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.7\n",
    "\n",
    "Let the scalar $q$ be given by the quadratic form\n",
    "\n",
    "$$q = \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{x}$ is $n \\times 1$, $\\mathbf{A}$ is $n \\times n$, and $\\mathbf{A}$ does not depend on $\\mathbf{x}$. Prove that\n",
    "\n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{x}} = \\mathbf{x}^\\top (\\mathbf{A} + \\mathbf{A}^\\top)$$\n",
    "    \n",
    "and that (trivially) when $\\mathbf{A}$ is a symmetric matrix\n",
    "    \n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{x}} = 2 \\mathbf{x}^\\top \\mathbf{A}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.8\n",
    "\n",
    "Let the scalar $q$ be defined by\n",
    "\n",
    "$$q = \\mathbf{y}^\\top \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{y}$ is $n \\times 1$, $\\mathbf{x}$ is $n \\times 1$, and both $\\mathbf{x}$ and $\\mathbf{y}$ are functions of the vector $\\mathbf{z}$. Prove that (_hint: use the chain rule_)\n",
    "    \n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{z}} = \\mathbf{x}^\\top \\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{z}} + \\mathbf{y}^\\top \\dfrac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}}$$\n",
    "    \n",
    "and that (trivially) for $\\mathbf{y} = \\mathbf{x}$\n",
    "    \n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{z}} = 2 \\mathbf{x}^\\top \\dfrac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.9\n",
    "\n",
    "Let the scalar $q$ be defined by\n",
    "$$q = \\mathbf{y}^\\top \\mathbf{A} \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{y}$ is $m \\times 1$, $\\mathbf{x}$ is $n \\times 1$, $\\mathbf{A}$ is $m \\times n$, both $\\mathbf{y}$ and $\\mathbf{x}$ are functions of $\\mathbf{z}$, and $\\mathbf{A}$ does not depend on $\\mathbf{z}$. Prove that (_hint: notice that we can write $\\mathbf{w}^\\top = \\mathbf{y}^\\top \\mathbf{A}$ and use the chain rule_) \n",
    "\n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{z}} = \\mathbf{x}^\\top \\mathbf{A}^\\top \\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{z}} + \\mathbf{y}^\\top \\mathbf{A} \\dfrac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}}$$\n",
    "    \n",
    "and (trivially) that when $\\mathbf{y} = \\mathbf{x}$\n",
    "    \n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{z}} = \\mathbf{x}^\\top (\\mathbf{A} + \\mathbf{A}^\\top) \\dfrac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}}$$\n",
    "    \n",
    "and that (trivially) when $\\mathbf{A}$ is symmetric\n",
    "    \n",
    "$$\\dfrac{\\partial q}{\\partial \\mathbf{z}} = 2 \\mathbf{x}^\\top \\mathbf{A} \\dfrac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.10\n",
    "\n",
    "Let $\\mathbf{A}$ be a nonsingular $m \\times m$ matrix whose elements are a function of the scalar parameter $q$, prove that (_hint: start with $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$_)\n",
    "\n",
    "$$\\dfrac{\\partial \\mathbf{A}^{-1}}{\\partial q} = - \\mathbf{A}^{-1} \\dfrac{\\partial \\mathbf{A}}{\\partial q} \\mathbf{A}^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.11\n",
    "\n",
    "Let $\\mathbf{A}$ be an $n \\times n$ matrix whose elements are $a_{ij}$ and $\\mathbf{B}$ and $n \\times n$ matrix whose elements are denoted $b_{ij}$. Prove that\n",
    "\n",
    "$$\\dfrac{\\partial \\mathrm{Tr}(\\mathbf{A}\\mathbf{B})}{\\partial a_{ij}} = b_{ij}$$\n",
    "\n",
    "or, in more compact notation\n",
    "    \n",
    "$$\\dfrac{\\partial \\mathrm{Tr}(\\mathbf{A}\\mathbf{B})}{\\partial \\mathbf{A}} = \\mathbf{B}^\\top$$\n",
    "    \n",
    "Thus prove the following identities\n",
    "    \n",
    "$$\\dfrac{\\partial \\mathrm{Tr}(\\mathbf{A}^\\top \\mathbf{B})}{\\partial \\mathbf{A}} = \\mathbf{B}$$\n",
    "   \n",
    "    \n",
    "$$\\dfrac{\\partial \\mathrm{Tr}(\\mathbf{A})}{\\partial \\mathbf{A}} = \\mathbf{I}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Eigenvector decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector equation for an $M \\times M$ matrix $\\mathbf{A}$ is\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{u}_i = \\lambda_i \\mathbf{u}_i$$\n",
    "    \n",
    "where $\\lambda_i$ and $\\mathbf{u}_i$ are the i-th eigenvalue and eigenvector of $\\mathbf{A}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "Explain what is the **rank** of a matrix? Then, what is the rank of an $M \\times M$ real symmetric matrix and why? Finally, using this fact prove that\n",
    "\n",
    "$$\\mathbf{D} = \\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}$$\n",
    "    \n",
    "where $\\mathbf{D}$ is a diagonal matrix whose elements are the sorted eigenvalues of $\\mathbf{A}$, and $\\mathbf{P} = (\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_n)$ is a matrix whose columns are the eigenvectors of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Now, consider the following $4 \\times 4$ real and symmetric (Hermitian) matrix\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix} \n",
    "0.3849649633449483 & -0.0115127847644803 & 0.0289786258220766 & 0.0284870976446458 \\\\\n",
    "-0.0115127847644803 & 0.1225807354564958 & -0.0179079588168552 & -0.0219347232995010 \\\\\n",
    "0.0289786258220766 & -0.0179079588168552 & 0.3789000179970620 & 0.0751165423974623 \\\\\n",
    "0.0284870976446458 & -0.0219347232995010 & 0.0751165423974623 & 0.5029358112556526\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using the appropriate [numpy](https://numpy.org/) function, compute numerically the eigenvalues and eigenvectors of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Using the appropriate [numpy](https://numpy.org/) function, demonstrate numerically that the characteristic equation\n",
    "\n",
    "$$|\\mathbf{A} - \\lambda_i \\mathbf{I}| = 0$$\n",
    "\n",
    "holds true for all $\\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "\n",
    "Using the appropriate [numpy](https://numpy.org/) function, compute the inverse matrix $\\mathrm{A}^{-1}$. Why is $\\mathrm{A}^{-1}$ guaranteed to exist in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5\n",
    "\n",
    "Demonstrate **numerically** that the following dyadic summations are true:\n",
    "\n",
    "$$\\mathbf{A} = \\sum_{i=1}^{M} \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$$\n",
    "    \n",
    "and\n",
    "    \n",
    "$$\\mathbf{A}^{-1} = \\sum_{i=1}^{M} \\dfrac{1}{\\lambda_i} \\mathbf{u}_i \\mathbf{u}_i^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.6\n",
    "Using the appropriate [numpy](https://numpy.org/) function, compute the determinant $|\\mathbf{A}|$. Then, demonstrate numerically that\n",
    "\n",
    "$$|\\mathbf{A}| = \\prod_{i=1}^{M} \\lambda_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.7\n",
    "Using the appropriate [numpy](https://numpy.org/) function, compute the trace $\\mathrm{Tr}(\\mathbf{A})$. Then, demonstrate numerically that\n",
    "\n",
    "$$\\mathrm{Tr}(\\mathbf{A}) = \\sum_{i=1}^{M} \\lambda_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Polynomial fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following Python functions that generates synthetic data of the form\n",
    "\n",
    "$$t = f(x) + \\epsilon$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$f(x) = \\sin(2 \\pi x) + \\sin(4 \\pi x)$$\n",
    "\n",
    "and\n",
    "    \n",
    "$$\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def func(x):\n",
    "    fmax = np.sqrt((207 + 33**1.5)/128)\n",
    "    return (np.sin(2 * np.pi * x) + np.sin(4 * np.pi * x))/fmax\n",
    "\n",
    "def create_toy_data(func, sample_size, std):\n",
    "    x = np.linspace(0, 1, sample_size)\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_toy_data(func, 10, 0.33)\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "y_test = func(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "\n",
    "Using a plotting library of your choosing (e.g. matplotlib) make a graph showing \n",
    "\n",
    "- the function $f(x)$ i.e. `(x_test, y_test)`, shown as a solid line.   \n",
    "- the training data `(x_train, y_train)`, shown as circles.  \n",
    "\n",
    "Add a legend to the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "Now consider the polynomial function\n",
    "\n",
    "$$y(x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_m x^M = \\mathbf{w}^\\top \\mathbf{\\phi}(x)$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$\\mathbf{\\phi}(x) = (1, x, x^2, \\dots, x^M)$$\n",
    "\n",
    "is a polynomoual _feature vector_ of order $M$.\n",
    "    \n",
    "Now consider a dataset of inputs $\\mathbf{x} = \\{ x_1, x_2, \\dots, x_N\\}$ with corresponding target vectors $\\mathbf{t} = \\{ t_1, t_2, \\dots, t_N\\}$. The sum-of-squares error function for the linear regression problem is\n",
    "    \n",
    "$$\\begin{aligned}\n",
    "E(\\mathbf{w}) &= \\dfrac{1}{2} \\sum_{n=1}^N \\{t_n - y(x, \\mathbf{w})\\}^2 \\\\ &= \\dfrac{1}{2} \\sum_{n=1}^N \\{ t_n- \\mathbf{w}^\\top \\mathbf{\\phi}(x_n)\\}^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "Solving for $\\mathbf{w}$ (we will do this later in the course!) we obtain\n",
    "\n",
    "$$\\mathbf{w}^* = \\mathbf{\\Phi}^\\dagger \\mathbf{t}$$\n",
    "    \n",
    "where $\\mathbf{\\Phi}^\\dagger = (\\mathbf{\\Phi}^\\top\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^T$ is the Moore-Pensore pseudo-inverse of the _design matrix_ $\\mathbf{\\Phi}$, defined as:\n",
    "    \n",
    "$$\\mathbf{\\Phi} = \\begin{pmatrix}\\phi_0(x_1) & \\phi_1(x_1) & \\dots & \\phi_M(x_1) \\\\ \\phi_0(x_2) & \\phi_1(x_2) & \\dots & \\phi_M(x_2) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\phi_0(x_N) & \\phi_1(x_N) & \\dots & \\phi_M(x_N) \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "Write a Python function that can generate the design matrix $\\mathbf{\\Phi}$ for a dataset $\\mathbf{x} = \\{ x_1, x_2, \\dots, x_N\\}$ and polynomial features $\\mathbf{\\phi}(x) = (1, x, x^2, \\dots, x^M)$, of the following form\n",
    "\n",
    "```python\n",
    "def build_design_matrix(xdata, M):\n",
    "    # write the missing code here to generate design matrix X\n",
    "    return X\n",
    "```\n",
    "\n",
    "where `xdata` is the array of inputs $\\mathbf{x} = \\{ x_1, x_2, \\dots, x_N\\}$ and $M$ is the order of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "Then we define the following **class** for linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    adapted from https://github.com/ctgk/PRML/blob/master/prml/linear/linear_regression.py\n",
    "    Linear regression model\n",
    "    y = X @ w\n",
    "    t ~ N(t|X @ w, var)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X:np.ndarray, t:np.ndarray):\n",
    "        \"\"\"\n",
    "        perform least squares fitting\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, D) np.ndarray\n",
    "            training independent variable\n",
    "        t : (N,) np.ndarray\n",
    "            training dependent variable\n",
    "        \"\"\"\n",
    "        self.w = np.linalg.pinv(X) @ t\n",
    "\n",
    "    def predict(self, X:np.ndarray, return_std:bool=False):\n",
    "        \"\"\"\n",
    "        make prediction given input\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, D) np.ndarray\n",
    "            samples to predict their output\n",
    "        return_std : bool, optional\n",
    "            returns standard deviation of each predition if True\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N,) np.ndarray\n",
    "            prediction of each sample\n",
    "        y_std : (N,) np.ndarray\n",
    "            standard deviation of each predition\n",
    "        \"\"\"\n",
    "        y = X @ self.w\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the mode as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 6\n",
    "model = LinearRegression()\n",
    "X_train = build_design_matrix(x_train, M)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = build_design_matrix(x_test, M)\n",
    "y_fit = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "Using a plotting library of your choosing (e.g. matplotlib) make a graph showing \n",
    "\n",
    "- the function $f(x)$ i.e. `(x_test, y_test)`, shown as a solid line  \n",
    "- the training data `(x_train, y_train)`, shown as circles  \n",
    "- the line of best fit `(x_test, y_fit)`, shown as a solid line\n",
    "\n",
    "Add a legend to the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4\n",
    "\n",
    "Generate 4 plots as in Q. 3.3, for different polynomial orders $M=3, 6, 9, 12$. Comments on what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "Then we define the following **class** for regularized linear regression (ridge regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/ctgk/PRML/blob/master/prml/linear/ridge_regression.py\n",
    "    Ridge regression model\n",
    "    w* = argmin |t - X @ w| + alpha * |w|_2^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha:float=1.):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X:np.ndarray, t:np.ndarray):\n",
    "        \"\"\"\n",
    "        maximum a posteriori estimation of parameter\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, D) np.ndarray\n",
    "            training data independent variable\n",
    "        t : (N,) np.ndarray\n",
    "            training data dependent variable\n",
    "        \"\"\"\n",
    "\n",
    "        eye = np.eye(np.size(X, 1))\n",
    "        self.w = np.linalg.solve(self.alpha * eye + X.T @ X, X.T @ t)\n",
    "\n",
    "    def predict(self, X:np.ndarray):\n",
    "        \"\"\"\n",
    "        make prediction given input\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, D) np.ndarray\n",
    "            samples to predict their output\n",
    "        Returns\n",
    "        -------\n",
    "        (N,) np.ndarray\n",
    "            prediction of each input\n",
    "        \"\"\"\n",
    "        return X @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5\n",
    "\n",
    "Generate 4 plots as in Q. 3.4, but perform the fits using RidgeRegression for fixed polynomial order $M=12$ but different regularization parameters $\\alpha = 10^{-6}, 10^{-4}, 10^{-2}, 1$. Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
