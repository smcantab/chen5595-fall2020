{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW focuses on the expectation maximization (EM) algorithm for mixtures of Gaussians and K-means. It consists only of coding exercises. I provide several definitions and hints throughout that will help you.  If you are confused about anything, do not panic, send us a message on Piazza! We are here to help you learn.\n",
    "\n",
    "Please answer the numerical exercises using an ipython notebook **in [Google Colab](https://colab.research.google.com/)** (provide the link with your handwritten homework). **Please answer each coding problem  in a different cell**.\n",
    "\n",
    "You can also view this ipython notebook in your browser through [binder](https://mybinder.org/v2/gh/smcantab/chen5595-fall2020/fe832bceeec8c088ddd726c9cace42264a674c20?filepath=homework%2Fhw7-questions.ipynb) or through nbviewer [link](https://nbviewer.jupyter.org/github/smcantab/chen5595-fall2020/blob/master/homework/hw7-questions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization for Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _multivariate Gaussian mixture_ distribution with $K$ components is defined as\n",
    "\n",
    "$$p(x|\\mathbf{\\pi}, \\mathbf{\\mu}, \\Sigma) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k, \\Sigma_k) \\label{eq:mgm} \\tag{1}$$\n",
    "\n",
    "We also define a latent vector $\\mathbf{z}$ such that $z_k=1$ if an observation was drawn from component $k$ and all other elements of the vector are zero. For each of N observations $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\}$ there is a latent vector $\\{\\mathbf{z}_1, \\dots, \\mathbf{z}_N \\}$, defining the _complete data set_ $\\{\\mathbf{X}, \\mathbf{Z}\\}$, where\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix} \n",
    "\\mathbf{x}_1^\\top \\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf{x}_N^\\top\n",
    "\\end{pmatrix}\n",
    "~~\\text{and}~~\n",
    "\\mathbf{Z} = \n",
    "\\begin{pmatrix} \n",
    "\\mathbf{z}_1^\\top \\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf{z}_N^\\top\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The log-likelihood function of the mixture of Gaussians then is\n",
    "\n",
    "$$\\log p(\\mathbf{X}|\\mathbf{\\pi}, \\mathbf{\\mu}, \\Sigma) =  \\sum_{n=1}^N \\log \\left\\{ \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\Sigma_k) \\right\\}$$\n",
    "\n",
    "The goal of the EM algorithm is to maximize the likelihood function with respect to the parameters $\\mathbf{\\pi}, \\mathbf{\\mu}, \\Sigma$. The steps of the EM algorithm are as follows:\n",
    "\n",
    "1. Initialize the means $\\mu_k$, covariances $\\Sigma_k$ and mixing coefficients $\\pi_k$.\n",
    "\n",
    "2. **E step**. Evaluate the responsibilities using the current parameter values\n",
    "\n",
    "$$\\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j, \\Sigma_j)}$$\n",
    "\n",
    "3. **M step**. Re-estimate the parameters using the current responsibilities (just computed in step 2)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{\\mu}_k^{\\mathrm{new}} &= \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) \\mathbf{x}_n\\\\\n",
    "\\Sigma_k^{\\mathrm{new}} &= \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{\\mathrm{new}}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{\\mathrm{new}})^\\top\\\\\n",
    "\\pi_k^{\\mathrm{new}} &= \\frac{N_k}{N}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$N_k = \\sum_{n=1}^N \\gamma(z_{nk})$$\n",
    "\n",
    "4. Iterate the E and M steps until the parameters $\\mathbf{\\pi}, \\mathbf{\\mu}, \\Sigma$ are no longer changing (up to some tolerance).\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "Implement the EM algorithm for mixtures of Gaussians following the description above, your code should have the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateGaussianMixture(RandomVariable):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_components,\n",
    "                 mu=None,\n",
    "                 cov=None,\n",
    "                 coef=None):\n",
    "        \"\"\"\n",
    "        Constructor of mixture of Gaussians\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_components : int\n",
    "            number of gaussian component\n",
    "        mu : (n_components, ndim) np.ndarray\n",
    "            mean parameter of each gaussian component\n",
    "        cov : (n_components, ndim, ndim) np.ndarray\n",
    "            variance parameter of each gaussian component\n",
    "        coef : (n_components,) np.ndarray\n",
    "            mixing coefficients\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.mu = mu\n",
    "        self.cov = cov\n",
    "        self.coef = coef\n",
    "        \n",
    "    def _gauss(self, X):\n",
    "        # returns K Gaussians evaluated on data set of observations X\n",
    "    \n",
    "    def _expectation(self, X):\n",
    "        # performs the E step\n",
    "        # returns the responsibilities resps\n",
    "        return resps\n",
    "    \n",
    "    def _maximization(self, X, resps):\n",
    "        # performs the M step\n",
    "        # updates self.mu, self.cov, self.coef\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # write a `while` statement that performs self._expectation and self._maximization\n",
    "        # until the parameters stop changing (you can use the function np.allclose)\n",
    "    \n",
    "    def pdf(self, X):\n",
    "        # returns the probability density function for the fitted gaussian mixture\n",
    "        \n",
    "    def responsibilities(self, X):\n",
    "        return self._expectation(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset `mgm-hw7.txt` and produce a scatter plot using a plotting library of your choosing (e.g. matplotlib). Select suitable initialization conditions (by inspection, or using the output of K-means from Problem 3) for the EM algorithm and fit the data using your implementation of a Gaussian mixture model (for $K=2$) and output the parameters every few iterations. Then for $6$ representative iterations (include the initial and final ones) produce (i) a countour plot of the fitted pdf and (ii) superimpose the scatter, (iii) coloring the points according to their responsibility (for simplicity do this in a binary fashion, so that you color the points according to the largest responsibility coefficient).\n",
    "\n",
    "Produce also a plot of the value of the log-likelihood as a function of number of iterations\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "Do the same as above using the readily available implementation [`sklearn.mixture.Gaussian`](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture), but only show the initial and final pdfs.\n",
    "\n",
    "## K-means Clustering\n",
    "\n",
    "The K-means clustering algorithm can be thought of as a non-probabilistic limit of the Gaussian mixture model, where the responsibilities become binary indicators $r_{nk} \\in \\{0, 1\\}$, where $k=1, \\dots, K$ describing which of the $K$ clusters the data point $\\mathbf{x}_n$ is assigned to, so that if point $\\mathbf{x}_n$ is assigned to cluster $k$ then $r_{nk}=1$ and $r_{nj}=0$ for $j \\neq k$. K-means clustering minimizes the _distortion measure_ given by\n",
    "\n",
    "$$J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} |\\mathbf{x}_n - \\mathbf{\\mu}_k|^2$$\n",
    "\n",
    "which represents the sum of the squares of the distances of each data point to the cluster prototype vector $\\mu_k$. The goal of EM for K-means is to find the values of $r_{nk}$ and $\\mathbf{\\mu}_k$. The steps of the EM algorithm are as follows:\n",
    "\n",
    "1. Initialize the means $\\mu_k$.\n",
    "\n",
    "2. **E step**. Evaluate the binary indicators $r_{nk}$ using the current parameter values\n",
    "\n",
    "$$r_{nk} =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } k = \\text{arg min}_j |\\mathbf{x}_n - \\mathbf{\\mu}_j|^2\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "3. **M step**. Re-estimate the means using the binary indicators computed in 1\n",
    "\n",
    "$$\\mathbf{\\mu}_k = \\frac{\\sum_{n=1}^N r_{nk}\\mathbf{x}_{n}}{\\sum_{n=1}^N r_{nk}}$$\n",
    "\n",
    "4. Iterate the E and M steps until J or $\\mathbf{\\mu}$ are no longer changing (up to some tolerance).\n",
    "\n",
    "Because each iteration of the algorithm reduces the distortion measure J, convergence is assured.\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "Implement the K-means algorithm following the description above, your code should have the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    \"\"\"\n",
    "        Constructor of K-mean\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_clusters : int\n",
    "            number of cluster\n",
    "        mu : (n_components, ndim) np.ndarray\n",
    "            center of mass of each cluster\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, centers):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centers = centers\n",
    "    \n",
    "    def _expectation(self, X):\n",
    "        # performs the E step\n",
    "        # returns the binary indicators r_nk \n",
    "        return R\n",
    "    \n",
    "    def _maximization(self, X, resps):\n",
    "        # performs the M step\n",
    "        # updates self.centers\n",
    "    \n",
    "    def fit(self, X, iter_max=100):\n",
    "        # perform the E and M steps of the K-means algorithm\n",
    "        # for up to itera_max\n",
    "    \n",
    "    def responsibilities(self, X):\n",
    "        return self._expectation(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset `mgm-hw7.txt`. Select random initial positions for the centers and fit the data using your implementation of the K-means algorithm (for $K=2$). Output the centers every few iterations. Then for 6 representative iterations (include the initial and final ones) produce (i) a scatter plot with crosses corresponding to the location of the centers, (ii) coloring the points according to their binary indicators $r_{nk}$. \n",
    "\n",
    "Produce also a plot of the value of the distortion measure as a function of number of iterations.\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "Do the same as above using the readily available implementation [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), but only show the initial and final iterations. \n",
    "\n",
    "Try this for $K=1,2,3,4$, do you always get the same result when initializing the centers with different random values? Comment on your observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
