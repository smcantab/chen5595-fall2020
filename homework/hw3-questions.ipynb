{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW focuses on Bayesian reasoning. It consists of both hand-written and coding exercises. I provide several definitions and hints throughout that will help you.  If you are confused about anything, do not panic, send us a message on Piazza! We are here to help you learn.\n",
    "\n",
    "Please answer all the questions involving proofs/demonstrations using pen and paper, and all the numerical exercises using an ipython notebook **in [Google Colab](https://colab.research.google.com/)** (provide the link with your handwritten homework). **Please answer each coding problem  in a different cell**.\n",
    "\n",
    "You can also view this ipython notebook in your browser through binder by following this [link](https://hub.gke2.mybinder.org/user/smcantab-chen5595-fall2020-b42kar43/tree/homework) and going to `homework/hw3-questions.ipynb`, or directly at this nbviewer [link](https://nbviewer.jupyter.org/github/smcantab/chen5595-fall2020/blob/master/homework/hw3-questions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "State Bayes rule, define all the terms and explain intuitively what their meaning is in a classification problem with inputs $\\mathbf{x}$ belonging to classes $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "After your yearly checkup, the doctor comes back with bad news. The news is that you tested positive for a serious disease, and that the test is $99\\%$ accurate (i.e., the probability of testing positive ($T=1$) given that you have the disease ($D=1$) is $p(T=1|D=1) = 0.99$, as is the probability of testing negative ($T=0$) given that you don’t have the disease ($D=0$), i.e. $P(T=0|D=0)=0.99$. The doctor also tells you that this is a rare disease, striking only one in 10,000 people so that $p(D=1)=10^{-4}$. Having taken CHEN 5595 you know that this is great news and do not let this test result discourage you, you proceed to show the doctor that the chance that you actually have the disease given that you tested positive is $p(D=1|T=1)\\approx 0.01$. Show your calculations and compute the accurate value of $p(D=1|T=1)$, then comment on the meaning of this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "My neighbor has two children. Assuming that the gender of a child is like a coin flip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other possibilities—two boys or two girls—have probabilities 1/4 and 1/4. (_Hint: construct a table with the fot possibilies G G, G B, B G, B B, and assign to each probability 1/4_)\n",
    "\n",
    "1. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is a girl? _Hint: compute_ $p(N_G=1| N_B \\geq 1)$ _using Bayes theorem_.\n",
    "\n",
    "2. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability that the other child is a girl? _Hint: compute:_ $p(X=G|Y=B)$ _using Bayes Theorem, where Y indicates the identity of the observed child and X the identity of the other child_. \n",
    "\n",
    "#### Comment:\n",
    "\n",
    "Tom Minka has written the following about these results: \n",
    "\n",
    "This seems like a paradox because it seems that in both cases we could condition on the fact that ”at least one child is a boy.” But that is not correct; you must condition on the event actually observed, not its logical implications. In the first case, the event was ”He said yes to my question.” In the second case, the event was ”One child appeared in front of me.” The generating distribution is different for the two events. Probabilities reflect the number of possible ways an event can happen, like the number of roads to a town. Logical implications are further down the road and may be reached in more ways, through different towns. The different number of ways changes the probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Let $H \\in \\{1,\\dots,K\\}$ be a discrete random variable, and let $e_1$ and $e_2$ be the observed values of two other random variable $E_1$ and $E_2$. Suppose we wish to calculate  $P(H=1|e_1,e_2)$. Which of the following sets of numbers (it can be more than one set) are sufficient for the calculation? _Hint: use Bayes rule_\n",
    "\n",
    "1. $P(e_1, e_2)$, $P(H)$, $P(e_1|H)$, $P(e_2|H)$\n",
    "2. $P(e_1, e_2)$, $P(H)$, $P(e_1, e_2|H)$\n",
    "3. $P(e_1|H)$, $P(e_2|H)$, $P(H)$\n",
    "\n",
    "Did you need all the values in the set (or sets) that you chose?\n",
    "\n",
    "Now suppose we assume that $E_1 \\bot E_2 | H$ meaning that $E_1$ and $E_2$ are conditionally independent given $H$. First, write down an equation defining the property of conditional independence and explaining its meaning intuitively with an example. Second, which of the above three sets (it can be more than one set) are sufficient now? _Hint: use Bayes rule and the property of conditional independence_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "Suppose you visit Las Vegas for the holidays, and come across a very strange coin in one of the casinos. You decide to set up an experiment to determine the bias-weighting $\\mu$ i.e. the average outcome of an infinite number of coin flips. You decide to adopt a Bayesian approach to quantify $\\mu$ and your confidence in your result after only a finite number of coin flips. From Bayes theorem we know that\n",
    "\n",
    "$$p(\\mu | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mu)p(\\mu)}{p(\\mathcal{D})} \\label{eq:bayes} \\tag{1}$$\n",
    "\n",
    "The prior $p(\\mu)$ on the right hand side of Eq. \\eqref{eq:bayes} represents what we know about the coin given only the information that we are dealing with \"a strange coin from Las Vegas\". We are going to encode our belief into a prior called _beta_ distribution, given by\n",
    "\n",
    "$$\\mathrm{Beta}(\\mu|a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\mu^{a-1}(1-\\mu)^{b-1} \\label{eq:prior} \\tag{2}$$\n",
    "\n",
    "with hyperparamters $a$ and $b$ and $\\Gamma(x)$ is the gamma function. The mean and variance of the Beta distribution are given by\n",
    "\n",
    "$$\\mathbb{E}[\\mu] = \\frac{a}{a+b}$$\n",
    "\n",
    "$$\\mathrm{var}[\\mu] = \\frac{ab}{(a+b)^2(a+b+1)}$$\n",
    "\n",
    "and its mode is\n",
    "\n",
    "$$\\mathrm{mode}[\\mu] = \\frac{a-1}{a+b-2}$$\n",
    "\n",
    "**Q1.** The beta distribution is hard to compute directly for large (a, b), in order to produce a stable implementation write it as $p(\\mu|a, b) = \\exp[\\log(p(\\mu|a, b))]$ and use the `scipy.special.gammaln` to evaluate the natural logarithm of the gamma function. This will compute the ratio of gamma functions without requiring the manipulation of large numbers that may overflow. Then, using a plotting library of your choosing (e.g. matplotlib) make line plots of the Beta distribution for the following choices for $(a,b) = (0.1, 0.1), (1, 1), (2, 3), (8, 4), (20, 2), (50, 5)$.\n",
    "\n",
    "The prior state of knowledge, or ignorance, is modified by the data through the _likelihood function_ $p(\\mathcal{D}|\\mu)$. It is a meaasure of the chance that we would have obtained the data that we actually observed if the value of the bias-weighting $\\mu$ was given. If we assume that the coin flips were independent events, so that the outcome of one did not influence that of the others, then the probability of obtaining the data \"$m$ heads in $N$ tosses\" is given by the Binomial distribution\n",
    "\n",
    "$$\\mathrm{Bin}(m|N, \\mu) = \\binom{N}{m} \\mu^m (1-\\mu)^{N-m} \\label{eq:likel} \\tag{3}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\binom{N}{m} \\equiv \\frac{N!}{(N-m)!m!}$$ \n",
    "\n",
    "is called the binomial coefficient, and it is the number of ways of choosing $m$ objects out of a total of $N$ identical objects. The binomial distribution has mean and variance\n",
    "\n",
    "$$\\mathbb{E}[m] = N\\mu$$\n",
    "\n",
    "$$\\mathrm{var}[m] = N \\mu (1-\\mu)$$\n",
    "\n",
    "**Q2.** Using a plotting library of your choosing (e.g. matplotlib) make histograms of the binomial distribution for the following choices of parameters: $(N, \\mu) = (3, 0.25), (10, 0.25), (100, 0.25), (1000, 0.25)$.\n",
    "\n",
    "According to Eq. \\eqref{eq:bayes} the product of the prior and of the likelihood yields the posterior, up to a normalization constant called _model evidence_ or _marginal likelihood_. We chose the functional form of the prior to be of the form $\\mu^x(1-\\mu)^{1-x}$ so that its product with the likelihood (i.e. the posterior) will have the same functional form as the prior. This property is called _conjugacy_. Concretely our choice of conjugate prior when multiplied with the likelihood yields the posterior\n",
    "\n",
    "$$p(\\mu|m, N, a, b) = \\frac{\\Gamma(a+N+b)}{\\Gamma(m+a)\\Gamma(N-m+b)} \\mu^{m+a-1} (1-\\mu)^{N-m+b-1} \\label{eq:post} \\tag{4}$$\n",
    "\n",
    "Since casinos can be rather dubious places, we should keep a very open mind about the nature of the coin, so to start we will assume a uniform prior.\n",
    "\n",
    "**Q3.** What choice of parameters $(a, b)$ for the prior yields a uniform distribution?\n",
    "\n",
    "**Q4.** Using a plotting library of your choosing (e.g. matplotlib) make line plots of the posterior for $N= 0, 1, 2, 3, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192$ coin flips stored in the file `coin_flips.txt` hosted in the course githup repository, [here](https://github.com/smcantab/chen5595-fall2020/tree/master/homework) (use the appropriate numpy function to import the data, heads are labelled as 1 and tails as 0).\n",
    "\n",
    "**Q5.** What can we say after $N=1$ coin flip? What about after $N=2, 3, 4, \\dots, 8192$ coin flips?\n",
    "\n",
    "The predictive distribution for the next coin flip, i.e. what we believe the probability that the next coin flip is going to be heads ($x=1$) after seeing $N$ coin flips is\n",
    "\n",
    "$$p(x=1|\\mathcal{D}) = \\int_0^1 p(x=1|\\mu) p(\\mu|\\mathcal{D}) \\mathrm{d}\\mu = \\int_0^1 \\mu p(\\mu|\\mathcal{D}) \\mathrm{d}\\mu = \\mathbb{E}[\\mu|\\mathcal{D}]$$\n",
    "\n",
    "**Q6.** Using the fact that the posterior, Eq. \\eqref{eq:post}, is a Beta distribution, and thus using the definition for the expectation of the Beta distribution reported above, provide an analytical result for $\\mathbb{E}[\\mu|\\mathcal{D}]$. Thus, for all $N$, show that $p(x=1|\\mathcal{D})$ computed analytically using the exact expression for $\\mathbb{E}[\\mu|\\mathcal{D}]$ matches the expectation computed by numerical integration (e.g. using Scipy) .\n",
    "\n",
    "**Q7.** Using the fact that the posterior, Eq. \\eqref{eq:post}, is a Beta distribution, and thus using the definition for the mode of the Beta distribution reported above, compute the most likely value that we are to observe for all $N$.\n",
    "\n",
    "A good way of expressing the reliability with which a paramter can be inferred for an asymmetric posterior probability density function is through a *confidence interval*. Since the area under the posterior between $\\mu_1$ and $\\mu_2$ is proprtional to how much we believe that $\\mu$ lies in that range, the *shortest interval* that encloses 95% of the area represents a sensible measure of the uncertainty of the estimate. Since the posterior in this problem has been normalized, we need to find $\\mu_1$ and $\\mu_2$ such that\n",
    "\n",
    "$$p(\\mu_1 \\leq \\mu \\leq \\mu_2 | \\mathcal{D}) = \\int_{\\mu_1}^{\\mu_2} p(\\mu|\\mathcal{D}) \\mathrm{d} \\mu \\approx 0.95$$\n",
    "\n",
    "where the difference $\\mu_2-\\mu_1$ is as small as possible.\n",
    "\n",
    "**Q8.** For all $N$ compute numerically the shortest 95% confidence intervals (to a reasonable accuracy) for the posterior. Then report in a table $\\mathrm{mode}[\\mu]$, the expectation $\\mathbb{E}[\\mu|\\mathcal{D}]$ along with the shortest 95% confidence intervals for all $N$. _Hint: do so in a brute force fashion, by integrating starting from a grid of initial points_ $\\mu_1$ _(less than the mode) and finding the corresponding_ $\\mu_2$ _for each. Then select the pair corresponding to the shortest interval._\n",
    "\n",
    "**Q9.** Is the coin fair? Explain your reasoning.\n",
    "\n",
    "Now let us say that you talk to your friend Alice about your experiment and the conclusions you drew. Alice strongly disagrees, she believes that casinos in Las Vegas cannot be trusted, so she flips the coin 10 times and gets 9 heads, and takes it as an indication that she was right and the coin cannot be fair. So she suggests to repeat the experiment with a prior that reflects her beliefs, i.e. a Beta prior with $(a, b) = (50, 5)$ (with expectation $\\mathbb{E}[\\mu] \\approx 0.9$).\n",
    "\n",
    "**Q10.** Answer questions 4-9 (except the analytical derivations) for this new choice of prior. Compare and comment the results obtained. Have your conclusions changed? What is the problem with Alice's approach, if any?\n",
    "\n",
    "To quantitatively asses which belief/model, yours or Alice, better describes the data after $N$ coin flips we can compute the Bayes factor\n",
    "\n",
    "$$K \\equiv \\frac{p(\\mathcal{D}|a=1, b=1)}{p(\\mathcal{D}|a=50, b=5)}$$\n",
    "\n",
    "$K>1$ means that your model is more strongly supported by the data under consideration than Alice's.\n",
    "\n",
    "**Q11.** Compute the Bayes factor to compare yours and Alice's \"models\" as a function of $N$. Which model is more strongly supported by the data? _Hint: the marginal-likelihood is given by the reciprocal of the normalization factor in Eq. \\eqref{eq:post}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
