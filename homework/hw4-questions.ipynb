{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW focuses on Shannon entropy and data compression. It consists of both hand-written and coding exercises. I provide several definitions and hints throughout that will help you.  If you are confused about anything, do not panic, send us a message on Piazza! We are here to help you learn.\n",
    "\n",
    "Please answer all the questions involving proofs/demonstrations using pen and paper, and all the numerical exercises using an ipython notebook **in [Google Colab](https://colab.research.google.com/)** (provide the link with your handwritten homework). **Please answer each coding problem  in a different cell**.\n",
    "\n",
    "You can also view this ipython notebook in your browser through [binder](https://mybinder.org/v2/gh/smcantab/chen5595-fall2020/2dffcc22fac8169d2f0b7a3af81b5ebde5e32c58?filepath=homework%2Fhw4-questions.ipynb) or through nbviewer [link](https://nbviewer.jupyter.org/github/smcantab/chen5595-fall2020/blob/master/homework/hw4-questions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Consider a vector $\\mathbf{x}$ of continuous variables with distribution $p(\\mathbf{x})$ and corresponding entropy $H[\\mathbf{x}]$. Suppose that we make a nonsingular linear transformation of $\\mathbf{x}$ to obtain a new variable $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$. Show that the corresponding entropy is given by $H[\\mathbf{y}] = H[\\mathbf{x}] + \\log |\\mathbf{A}|$ where $|\\mathbf{A}|$ denotes the determinant of $\\mathbf{A}$. _Hint: recall that the change of variable for probability distributions takes the form_ $p_y(y) = p_x(g(y)) |g'(y)|$.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Suppose we have a joint distribution $p(\\mathbf{x}, \\mathbf{y})$ from which we draw pairs of values of $\\mathbf{x}$ and $\\mathbf{y}$. If a value of $\\mathbf{x}$ is already known, then the additional information needed to specify the corresponding value of $\\mathbf{y}$ is given by $− \\log p(\\mathbf{y}|\\mathbf{x})$. Thus the average additional information needed to specify $\\mathbf{y}$ can be written as\n",
    "\n",
    "$$H[\\mathbf{y}|\\mathbf{x}] = -\\iint p(\\mathbf{y}, \\mathbf{x}) \\log p(\\mathbf{y}|\\mathbf{x}) \\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{x} \\label{eq:condent} \\tag{1}$$\n",
    "\n",
    "which is called the _conditional entropy_ of $\\mathbf{y}$ given $\\mathbf{x}$.\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "Using Eq. \\eqref{eq:condent} together with the product rule of probability, show that\n",
    "\n",
    "$$H[\\mathbf{x}, \\mathbf{y}] = H[\\mathbf{y}|\\mathbf{x}] + H[\\mathbf{x}] \\label{eq:id1} \\tag{2}$$\n",
    "\n",
    "where $H[\\mathbf{x}, \\mathbf{y}]$ is the differential entropy of $p(\\mathbf{x}, \\mathbf{y})$ and $H[\\mathbf{x}]$ is the differential entropy of the marginal distribution $p(\\mathbf{x})$.\n",
    "\n",
    "_Hint: start with the product rule (inside the log)_ $p(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{x})$\n",
    "\n",
    "### Definition\n",
    "\n",
    "Now consider the joint distribution between two sets of variables $\\mathbf{x}$ and $\\mathbf{y}$ given by $p(\\mathbf{x}, \\mathbf{y})$. If the sets of variables are independent, then their joint distribution will factorize into the product of their marginals $p(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x})p(\\mathbf{y})$. If the variables are not independent, we can gain some idea of whether they are ‘close’ to being independent by considering the Kullback-Leibler divergence between the joint distribution and the product of the marginals, given by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "I[\\mathbf{x},\\mathbf{y}] &\\equiv \\mathrm{KL}(p(\\mathbf{x}, \\mathbf{y})||p(\\mathbf{x})p(\\mathbf{y})) \\\\\n",
    "&= -\\iint p(\\mathbf{x}, \\mathbf{y}) \\log \\left (\\frac{p(\\mathbf{x})p(\\mathbf{y})}{p(\\mathbf{x}, \\mathbf{y})} \\right ) \\mathrm{d}\\mathbf{x}\\mathrm{d}\\mathbf{y}\n",
    "\\end{aligned} \\label{eq:mi} \\tag{3}$$\n",
    "\n",
    "which is called the _mutual information_ between the variables $\\mathbf{x}$ and $\\mathbf{y}$. From the properties of the Kullback-Leibler divergence, we see that $I[\\mathbf{x},\\mathbf{y}] \\geq 0$ with equality if, and only if, $\\mathbf{x}$ and $\\mathbf{y}$ are independent.\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "Using the sum and product rules of probability, show that the mutual information $I[\\mathbf{x}, \\mathbf{y}]$ in Eq. \\eqref{eq:mi} satisfies the relation\n",
    "\n",
    "$$I[\\mathbf{x}, \\mathbf{y}] = H[\\mathbf{x}] - H[\\mathbf{x}|\\mathbf{y}] = H[\\mathbf{y}] - H[\\mathbf{y}|\\mathbf{x}] \\label{eq:id2} \\tag{4}$$\n",
    "\n",
    "_Hint: start with the product rule (inside the log)_ $p(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{x})$\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "Using information diagrams (a type of Venn diagrams used to illustrate relationships between Shannon entropy measures) illustrate the identities in Eq. \\eqref{eq:id1} and Eq. \\eqref{eq:id2}.\n",
    "\n",
    "### Definition\n",
    "\n",
    "The entropy of a sequence of binary digits $\\mathbf{x}_b = (x_1, x_2, \\dots, x_N)$ sampled i.i.d. from a Bernoulli process with probability $p$, e.g. a sequence of coin flips where the probability of obtaining heads ($X=1$) is given by $p$ and the probability of obtained tails ($X=0$) is $1-p$, is given by the binary entropy function\n",
    "\n",
    "$$H_b(p) = - p \\log_2 p - (1-p) \\log_2 (1-p)$$\n",
    "\n",
    "where $0 \\log_2 0$ is taken to be $0$.\n",
    "\n",
    "The multiplicity of the same binary process is\n",
    "\n",
    "$$W_b = \\binom{N}{m} \\equiv \\frac{N!}{(N-m)!m!}$$\n",
    "\n",
    "where $N$ is the number of digits in the sequence, and $m$ is the number of events for which $X=1$.\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "Using the fact that $\\mathbb{E}[m] = N p$, **derive the binary entropy function** starting from $H_b = \\log W_b$ and using Stirling's approximation $\\log N! = N\\log N - N$. _Hint: start by replacing_ $m$ _with_ $\\mathbb{E}[m]$ _in the expression for the multiplicity $W_b$._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "The noiseless coding theorem (Shannon 1948) states that the shortest encoding of a sequence is given by its entropy. Thus, in the case of a sequence of binary digits $\\mathbf{x}_b = (x_1, x_2, \\dots, x_N)$ sampled i.i.d. from a Bernoulli process with probability $p$, the best encoding will require $H_b$ bits per character in $\\mathbf{x}$. This is a statement about data compression, saying that any data compression algorithm will produce an encoding $\\mathcal{L}(\\mathbf{x}_b) \\geq H_b$ that is at best as short as the entropy of the sequence. Algorithms that can fullfill this lower bound are said to be optimal.\n",
    "\n",
    "A popular lossless data compression algorithm that you have certainly used, possibly unkowingly, is DEFLATE (Katz 1991). DEFLATE is based on an optimal coding scheme due to Lempel and Ziv, followed by Huffman coding. This algorithm is what is being executed under the hood by popular lossesly compressed data formats like `.png` and `.zip`. Using the `zlib` python library we can write a set of functions to take an array of integers and return its compressed size in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import bitarray\n",
    "import zlib\n",
    "import copy\n",
    "\n",
    "def deflate_compress(raw_str, complevel=9):\n",
    "    \"\"\"\n",
    "    compresses a bytes string using DEFLATE\n",
    "    \"\"\"\n",
    "    zipper = zlib.compressobj(complevel, zlib.DEFLATED, -15, 9, zlib.Z_DEFAULT_STRATEGY)\n",
    "    deflated = zipper.compress(raw_str)\n",
    "    deflated += zipper.flush()\n",
    "    return deflated\n",
    "\n",
    "def get_size_bytes(s):\n",
    "    \"\"\"\n",
    "    returns the size of a bytes string\n",
    "    \"\"\"\n",
    "    return sys.getsizeof(s) - sys.getsizeof('')\n",
    "\n",
    "def get_bytes_string(seq):\n",
    "    \"\"\"\n",
    "    returns bytes string\n",
    "    https://stackoverflow.com/questions/13676183/python-choose-number-of-bits-to-represent-binary-number\n",
    "    \"\"\"\n",
    "    nbits = np.ceil(np.log2(np.amax(seq) + 1))\n",
    "    fsyntax = '0{}b'.format(int(nbits))\n",
    "    ba = bitarray.bitarray(''.join([format(x, fsyntax) for x in seq]))\n",
    "    return ba.tobytes()\n",
    "\n",
    "def get_compressed_size_bytes(seq):\n",
    "    seq = seq.astype('uint8')\n",
    "    raw_str = get_bytes_string(seq)\n",
    "    # compress once\n",
    "    compr_str1 = deflate_compress(raw_str)\n",
    "    compr_size1 = get_size_bytes(compr_str1)\n",
    "    compr_size = get_size_bytes(compr_str1)\n",
    "    # try to compress further\n",
    "    compr_str2 = copy.deepcopy(compr_str1)\n",
    "    while True:\n",
    "        compr_str2 = deflate_compress(compr_str2)\n",
    "        if get_size_bytes(compr_str2) >= compr_size:\n",
    "            break\n",
    "        else:\n",
    "            compr_size = get_size_bytes(compr_str2)\n",
    "    return compr_size\n",
    "\n",
    "def get_compressed_size_bpc(x):\n",
    "    \"\"\"\n",
    "    returns the entropy of an array of digints in bits per character (bpc)\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    enc = get_compressed_size_bytes(x)\n",
    "    h = 8 * enc / N\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Start by writing a function of the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bernoulli_seq(N, p):\n",
    "    # write a function that generates a Bernoullis sequence of length N with x~Bern(x|p). \n",
    "    # Hint: you could use the fact that m=N*p together with the np.random.shuffle function \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for $N=10^4, 10^5, 10^6$ compute the compressed file size of Bernoulli sequences with $p \\in [0, 1]$ (e.g. for 20 values in that range). Compute also the exact result $H_b$, corresponding to the limit $N \\to\\infty$. Using a plotting library of your choosing, plot the 4 curves in one graph, add a legend, and label the axes. Comment on the results. \n",
    "\n",
    "Why is the curve symmetric with respect to $p=0.5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "\n",
    "One can generalize the results above to a sequence of more than 2 variables. So let us say that $x$ can take one of three values $\\{0, 1, 2\\}$, with probabilities $p_0$, $p_1$ and $p_2 = 1-(p_1+p_2)$, so we have only 2 parameters (similarly to the binomial case where we had 2 possible outcomes but only 1 parameter).\n",
    "\n",
    "Start by writing a function of the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mult3_seq(N, p0, p1):\n",
    "    assert p0 + p1 <= 1, \"p0 + p1 exceeds 1\" \n",
    "    m0 = int(N*p0)\n",
    "    m1 = int(N*p1)\n",
    "    m2 = N - m0 - m1\n",
    "    # write a function that generates a Multinomial sequence of length N with x taking\n",
    "    # 3 possible values {0, 1, 2} with probabilities p1, p2, p3. \n",
    "    # Hint: you could use the fact that m0=N*p0, m1=N*p1, m2=N*p2 together with \n",
    "    # the np.random.shuffle function \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below iterate over a 2D grid `h = np.zeros((50, 50))` for $p_0 \\in [0, 1]$ and $p_1 \\in [0, 1]$, generate sequences with the corresponding pairs of $p_0$ and $p_1$ and evaluate the the entropy of the sequences using the `get_compressed_size_bpc` function above, and update the corresponding entry for `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.zeros((50, 50))\n",
    "x, y = np.linspace(0, 1, 50), np.linspace(0, 1, 50)\n",
    "for i, p0 in enumerate(x):\n",
    "    for j, p1 in enumerate(y):\n",
    "        if p0 + p1 <= 1:\n",
    "            seq = gen_mult3_seq(1e4, p0, p1)\n",
    "            h[i, j] = get_compressed_size_bpc(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plot the result using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, 50), np.linspace(0, 1, 50))\n",
    "contours = plt.contour(xx, yy, h, 4, colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "plt.imshow(h, extent=[0, 1, 0, 1], origin='lower', alpha=0.75)\n",
    "plt.xlabel(r'$p_0$')\n",
    "plt.ylabel(r'$p_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain these numerical results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
