{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW is divided in two parts. The first part will help you become more comfortable with probabilities, probability distributions and their moments, maximum likelihood estimators and the significance of bias. It consists of both hand-written and coding exercises. The second part retraces the steps of HW1 but this time we will be using Bayesian regression. I provide several definitions and hints throughout that will help you.  If you are confused about anything, do not panic, send us a message on Piazza! We are here to help you learn.\n",
    "\n",
    "Please answer all the questions involving proofs/demonstrations using pen and paper, and all the numerical excercises using an ipython notebook.\n",
    "\n",
    "You can also view this ipython notebook in your browser through nbviewer by following this [link](https://nbviewer.jupyter.org/github/smcantab/chen5595-fall2020/blob/master/homework/hw1-questions.ipynb). If you want to be able to modify and execute this script in the cloud you can launch it in Binder either by clicking \"Execute on Binder\" on the top right of the page in nbviewer, or by following this [link](https://mybinder.org/v2/gh/smcantab/chen5595-fall2020/d8e260074b28d8fda3aa30a4ca4acf8fba84ee15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "\n",
    "Suppose that we have three coloured boxes $r$ (red), $b$ (blue), and $g$ (green). Box $r$ contains 3 apples ($a$), 4 oranges ($o$), and 3 limes ($l$), box $b$ contains 1 apple, 1 orange, and 0 limes, and box $g$ contains 3 apples, 3 oranges, and 4 limes. If a box is chosen at random with probabilities $p(r) = 0.2$, $p(b) = 0.2$, $p(g) = 0.6$, and a piece of fruit is picked from the box (with equal probability of selecting any of the items in the box), then:  \n",
    "\n",
    "1. Show that the marginal probability of selecting an apple is $p(a)=0.34$?\n",
    "\n",
    "2. If we observe that the selected fruit is in fact an orange, show that the probability that it came from the green box is $p(g|o)=0.5$ (_hint: use Bayes theorem, and compute the marginal probability $p(o)$ the same way you computed $p(a)$_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "The expectation of a function $f(x)$ under probability distribution $p(x)$ for a discrete random variable $x$ is defined as\n",
    "\n",
    "$$\\mathbb{E}[f(x)] = \\sum_x p(x)f(x) \\label{eq:expectation} \\tag{1}$$\n",
    "\n",
    "and similarly the expectation of a function $f(x,z)$ under probability distribution $p(x,z)$ is\n",
    "\n",
    "$$\\mathbb{E}[f(x,z)] = \\sum_{x} \\sum_{z} p(x, z) f(x,z) \\label{eq:expectation2} \\tag{2}$$\n",
    "\n",
    "For continuum random variables, the sums in Eqs. \\eqref{eq:expectation}-\\eqref{eq:expectation2} should be replaced by integrals, and the probability distribution, _e.g._ $p(x)$, should be taken to be a probability density function (PDF), so that\n",
    "\n",
    "$$\\mathbb{E}[f(x)] = \\int_{-\\infty}^{\\infty} p(x)f(x)\\mathrm{d}x \\label{eq:expectation3} \\tag{3}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathbb{E}[f(x, z)] = \\int_{-\\infty}^{\\infty} p(x, z)f(x, z)\\mathrm{d}x \\mathrm{d}z \\label{eq:expectation4} \\tag{4}$$\n",
    "\n",
    "The variance of a function $f(x)$ under probability distribution $p(x)$ is defined as\n",
    "\n",
    "$$\\mathrm{var}[f] = \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2] \\label{eq:variance} \\tag{5}$$\n",
    "\n",
    "The covariance of two random variables $x$ and $y$ is defined as\n",
    "\n",
    "$$\\mathrm{cov}[x,y] = \\mathbb{E}[xy] - \\mathbb{E}[x]\\mathbb{E}[y] \\label{eq:covariance} \\tag{6}$$\n",
    "\n",
    "### Question 1.2\n",
    "\n",
    "Using Eq. \\eqref{eq:variance} show that $\\mathrm{var}[f]$ can be written as\n",
    "\n",
    "$$\\mathrm{var}[f] = \\mathbb{E}[f(x)^2] - \\mathbb{E}[f(x)]^2 $$\n",
    "\n",
    "### Question 1.3\n",
    "\n",
    "Starting from Eq. \\eqref{eq:covariance} use Eq. \\eqref{eq:expectation2} to show that the covariance of two **independent** random variables $x$ and $y$ is zero (_hint: recall that the joint probability of $x$ and $y$ factorizes as the product of the marginal probability of $x$ and the marginal probability of $y$. Thus use Eq. \\eqref{eq:expectation2}, with $f(x,z)=xz$, to show that $\\mathbb{E}[xy]=\\mathbb{E}[x]\\mathbb{E}[y]$ for independent variables, and the result follows immediately. Recall that proabilities distributions sum to one_).\n",
    "\n",
    "### Question 1.4\n",
    "\n",
    "Suppose that the two continuous variables $x$ and $z$ are statistically **independent**. Show that the mean and variance of their sum satisfies\n",
    "\n",
    "$$\\mathbb{E}[x+z] = \\mathbb{E}[x] + \\mathbb{E}[z]$$\n",
    "\n",
    "$$\\mathrm{var}[x+z] = \\mathrm{var}[x] + \\mathrm{var}[z]$$\n",
    "\n",
    "_Hint: for the first identity write out the expectation as in Eq. \\eqref{eq:expectation4} and use the properties of independent random variables as in question 1.3. Recall that proability densities alone integrate to one. For the second identity start from Eq. \\eqref{eq:variance}, write the outer expectation as in Eq. \\eqref{eq:expectation4} and then expand the square, the cross term will integrate to zero, and the result should follow_.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For the case of a real-valued variable $x$, the Gaussian distribution is defined by\n",
    "\n",
    "$$\\mathcal{N}(x|\\mu, \\sigma) =  \\frac{1}{(2 \\pi \\sigma^2)^{1/2}} \\exp\\left\\{ -\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right\\} \\label{eq:gaussian} \\tag{7}$$\n",
    "\n",
    "which is governed by the _mean_ $\\mu$ and the _variance_ $\\sigma^2$. The average value (or first moment) of $x$ under a Gaussian distribution is\n",
    "\n",
    "$$\\mathbb{E}[x] = \\int_{-\\infty}^\\infty \\mathcal{N}(x|\\mu, \\sigma) x \\mathrm{d}x = \\mu \\label{eq:gmu} \\tag{8}$$\n",
    "\n",
    "and the second moment is\n",
    "\n",
    "$$\\mathbb{E}[x^2] = \\int_{-\\infty}^\\infty \\mathcal{N}(x|\\mu, \\sigma) x^2 \\mathrm{d}x = \\mu^2 + \\sigma^2 \\label{eq:gmom2} \\tag{9}$$\n",
    "\n",
    "From which we can verify that the variance is\n",
    "\n",
    "$$\\mathrm{var}[x] = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2 = \\sigma^2 \\label{eq:gvar} \\tag{10}$$\n",
    "\n",
    "Now, suppose that we have a dataset with observations $\\boldsymbol{\\mathsf{x}} = (x_1, x_2, \\dots, x_N)^\\top$ representing $N$ observations of the scalar variable $x$. We shall suppose that our observations are drawn **independently** from a Gaussian $\\mathcal{N}(x|\\mu, \\sigma)$ whose mean $\\mu$ and variance $\\sigma^2$ are unknown, and we would like to determine these parameters from the dataset. Because our data are _independent and identically distributed_ (i.i.d.) we can write the probability of the dataset given $\\mu$ and $\\sigma$ in the form\n",
    "\n",
    "$$p(\\boldsymbol{\\mathsf{x}}|\\mu, \\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(x_n|\\mu, \\sigma) \\label{eq:glikelihood} \\tag{11}$$\n",
    "\n",
    "which, when viewed as a function of $\\mu$ and $\\sigma^2$, it is the _likelihood function_ for a Gaussian.\n",
    "\n",
    "### Question 1.5\n",
    "\n",
    "Taking the natural logarithm of the likelihood function, Eq. \\eqref{eq:glikelihood}, derive an expression for the _log-likelihood_ ($\\ell$). Then by setting the derivatives of the log-likelihood function with respect to $\\mu$ and $\\sigma^2$ equal to zero verify that\n",
    "\n",
    "$$\\mu_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^{N} x_n \\label{eq:smean} \\tag{12}$$\n",
    "\n",
    "$$\\sigma^2_{\\mathrm{ML}} = \\frac{1}{N} \\sum_{n=1}^{N} (x_n - \\mu_{\\mathrm{ML}})^2 \\label{eq:svar} \\tag{13}$$\n",
    "\n",
    "which are the _sample mean_ and the _sample variance_, respectively. _Hint: first find $\\mu_\\mathrm{ML}$ from $\\partial \\ell / \\partial \\mu = 0$; then substitute $\\mu_{\\mathrm{ML}}$ for $\\mu$ into $\\partial \\ell / \\partial \\sigma^2 = 0$ to find $\\sigma^2_\\mathrm{ML}$_.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Consider two points $x_n$ and $x_m$ sampled from a Gaussian $\\mathcal{N}(x|\\mu, \\sigma)$. To compute the expectation $\\mathbb{E}[x_n x_m]$ we identify the two possible cases:\n",
    "\n",
    "1. When $m=n$ then $x_n x_m = x_n^2$ so that from Eq. \\eqref{eq:gmom2} we find $\\mathbb{E}[x_n ^2] = \\mu^2 + \\sigma^2$.\n",
    "\n",
    "2. When $m \\neq n$ using the **independence** of $x_n$ and $x_m$ we can write $\\mathbb{E}[x_n x_m] = \\mathbb{E}[x_n]\\mathbb{E}[x_m] = \\mu^2$, where we have used \\eqref{eq:gmu}.\n",
    "\n",
    "Combining these two results we can write\n",
    "\n",
    "$$ \\mathbb{E}[x_n x_m] = \\mu^2 + I_{nm} \\sigma^2 \\label{eq:gpexp} \\tag{14}$$\n",
    "\n",
    "where $I_{nm}$ satisfies $I_{nm} = 1$ if $n=m$ and $I_{nm} = 0$ if $n \\neq m$.\n",
    "\n",
    "### Question 1.6\n",
    "\n",
    "Starting from Eq. \\eqref{eq:smean} show that for a Gaussian the sample mean $\\mu_\\mathrm{ML}$ is equal to the mean $\\mu$:\n",
    "\n",
    "$$\\mathbb{E}[\\mu_{\\mathrm{ML}}] = \\mu$$\n",
    "\n",
    "Now, using Eq. \\eqref{eq:gpexp} show that\n",
    "\n",
    "$$\\mathbb{E}\\left[x_n \\sum_{m=1}^N x_m\\right] = \\mu^2 + \\frac{1}{N} \\sigma^2$$ \n",
    "\n",
    "and also that\n",
    "\n",
    "$$\\mathbb{E}\\left[\\frac{1}{N^2}\\sum_{m=1}^N \\sum_{l=1}^N x_mx_l\\right] = \\mu^2 + \\frac{1}{N} \\sigma^2$$ \n",
    "\n",
    "Then using these facts and Eq. \\eqref{eq:gpexp}, starting from Eqs. \\eqref{eq:smean}-\\eqref{eq:svar} show that the sample variance $\\sigma^2_{\\mathrm{ML}}$ and the variance $\\sigma$ are related by the expression:\n",
    "\n",
    "$$\\mathbb{E}[\\sigma^2_{\\mathrm{ML}}] = \\left( \\frac{N-1}{N} \\right) \\sigma^2$$\n",
    "\n",
    "_Hint: plug Eq. \\eqref{eq:smean} into \\eqref{eq:svar}, expand the square and compute the resulting expectation using Eq. \\eqref{eq:gpexp} and the two identities/facts above_.\n",
    "\n",
    "How can we obtain an unbiased estimator for the variance?\n",
    "\n",
    "### Question 1.7\n",
    "\n",
    "Suppose that the variance of a Gaussian is estimated using Eq. \\eqref{eq:svar} but with the maximum likelihood estimate $\\mu_{\\mathrm{ML}}$ replaced with the true value $\\mu$ of the mean, so that:\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\mathbb{E} \\left[\\frac{1}{N} \\sum_{n=1}^{N} (x_n - \\mu)^2 \\right]$$\n",
    "\n",
    "where we have denoted $\\hat{\\sigma}^2$ our estimate of the variance according to the estimator above. Verify that this estimator yields the true variance, so that\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\sigma^2$$\n",
    "\n",
    "(_Hint: start by expanding the square in the definition of $\\hat{\\sigma}^2$, then use Eq. \\eqref{eq:gmu} and Eq. \\eqref{eq:gpexp}_)\n",
    "\n",
    "### Question 1.8\n",
    "\n",
    "Using the appropriate [numpy](https://numpy.org/) function, generate one dataset of 1000 i.i.d. samples from a Gaussian $\\mathcal{N}(x|\\mu, \\sigma)$ with mean $\\mu=0$ and variance $\\sigma^2=1$. Fit the data using the Maximum Likelihood estimators derived in question 1.5. Then using a plotting library of your choosing (e.g. Matplotlib) plot a histogram of the raw data and the likelihood function (i.e. the fitted Gaussian).\n",
    "\n",
    "### Question 1.9\n",
    "\n",
    "Using the appropriate [numpy](https://numpy.org/) function, generate $M=10$ dataset each with $N=3$ i.i.d. samples from a Gaussian $\\mathcal{N}(x|\\mu, \\sigma)$ with mean $\\mu=0$ and variance $\\sigma^2=1$. Fit each dataset using the Maximum Likelihood estimators derived in question 1.5, and using a plotting library of your choosing (e.g. Matplotlib) plot the 10 likelihood functions (i.e. the fitted Gaussians). Then compute the average (over the 10 datasets) of the sample means $\\mu_{\\mathrm{ML}}$, sample variance $\\sigma^2_{\\mathrm{ML}}$ and the unbiased estimate $\\tilde{\\sigma}^2 = \\frac{N}{N-1}\\sigma^2_{\\mathrm{ML}}$.\n",
    "\n",
    "Now compute the average of the sample means $\\mu_{\\mathrm{ML}}$, sample variance $\\sigma_{\\mathrm{ML}}$ and the unbiased estimate $\\tilde{\\sigma}^2$ for $M=\\{10, 30, 100, 300, 1000, 3000, 10000, 30000\\}$ datasets each with $N=3$ i.i.d. samples. Using a plotting library of your choosing (e.g. Matplotlib) plot in a single figure: \n",
    "\n",
    "1. $|\\mu_{\\mathrm{ML}}-\\mu|$ vs. $1/M$\n",
    "2. $|\\sigma^2_{\\mathrm{ML}}-\\sigma^2|$ vs. $1/M$\n",
    "3. $|\\tilde{\\sigma}^2_{\\mathrm{ML}}-\\sigma^2|$ vs. $1/M$\n",
    "\n",
    "Use a log scale for the x-axis and add a legend. Explain your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Bayesian Polynomial Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following Python functions that generate synthetic data of the form\n",
    "\n",
    "$$t = f(x) + \\epsilon$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$f(x) = \\frac{1}{2}\\left( \\sin(2 \\pi x) - \\cos(8 \\pi x) \\right)$$\n",
    "\n",
    "and\n",
    "    \n",
    "$$\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def func(x):\n",
    "    return (np.sin(2*np.pi*x) - np.cos(8*np.pi*x)) / 2\n",
    "\n",
    "def create_toy_data(func, sample_size, std=0.25):\n",
    "    x = np.linspace(0, 1, sample_size)\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design matrix for the polynomial feature vectors of order M can be constructed using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_design_matrix(xdata, M):\n",
    "    ndof = M+1\n",
    "    X = np.zeros((xdata.size, ndof))\n",
    "    exponents = np.arange(ndof)\n",
    "    for i, x in enumerate(xdata):\n",
    "        X[i, :] = np.power(np.ones(ndof) * x, exponents) \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Now suppose we have a dataset with observations $\\boldsymbol{\\mathsf{x}} = (x_1, x_2, \\dots, x_N)^\\top$ and associated target values $\\boldsymbol{\\mathsf{t}} = (t_1, t_2, \\dots, t_N)^\\top$. The *predictive distribution* for unseen inputs $x$ can be written:\n",
    "\n",
    "$$p(t|x, \\boldsymbol{\\mathsf{x}}, \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta) = \\int p(t|x, \\mathbf{w}, \\beta) p(\\mathbf{w}| \\boldsymbol{\\mathsf{x}}, \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta) \\mathrm{d} \\mathbf{w}$$\n",
    "\n",
    "where the first term of the integrand represent the uncertainty (noise) about the target value $t$, which we assume to be Gaussian with known precision parameter $\\beta$ (recall that the precision is the inverse of the variance, i.e. $\\beta= 1/\\sigma^2$), and the second term is the posterior over the parameters $\\mathbf{w}$. The posterior is proportional to the prior over the parameters $\\mathbf{w}$, which is taken to be Gaussian with mean zero and precision $\\alpha$. \n",
    "\n",
    "_Yes, it is complicated, if you don't feel comfortable with it don't worry! If you wish, check out the lecture notes for some insight. As far as this homework goes, all you need to understand is that the predictive distribution for unseen data can be computed, and it depends on the hyperparameter $\\alpha$ which sets the width of the Gaussian prior, and $\\beta$ (the precision of the noise) which we treat as a known constant_.\n",
    "\n",
    "The predictive distribution thus defined can be computed analytically, and the result of that Bayesian computation is encapsulated into the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegression:\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/ctgk/PRML/blob/master/prml/linear/bayesian_regression.py\n",
    "    Bayesian regression model\n",
    "    w ~ N(w|0, alpha^(-1)I)\n",
    "    y = X @ w\n",
    "    t ~ N(t|X @ w, beta^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha:float=1., beta:float=1.):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.w_mean = None\n",
    "        self.w_precision = None\n",
    "\n",
    "    def _is_prior_defined(self) -> bool:\n",
    "        return self.w_mean is not None and self.w_precision is not None\n",
    "\n",
    "    def _get_prior(self, ndim:int) -> tuple:\n",
    "        if self._is_prior_defined():\n",
    "            return self.w_mean, self.w_precision\n",
    "        else:\n",
    "            return np.zeros(ndim), self.alpha * np.eye(ndim)\n",
    "\n",
    "    def fit(self, X:np.ndarray, t:np.ndarray):\n",
    "        \"\"\"\n",
    "        bayesian update of parameters given training dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, n_features) np.ndarray\n",
    "            training data independent variable\n",
    "        t : (N,) np.ndarray\n",
    "            training data dependent variable\n",
    "        \"\"\"\n",
    "\n",
    "        mean_prev, precision_prev = self._get_prior(np.size(X, 1))\n",
    "\n",
    "        w_precision = precision_prev + self.beta * X.T @ X\n",
    "        w_mean = np.linalg.solve(\n",
    "            w_precision,\n",
    "            precision_prev @ mean_prev + self.beta * X.T @ t\n",
    "        )\n",
    "        self.w_mean = w_mean\n",
    "        self.w_precision = w_precision\n",
    "        self.w_cov = np.linalg.inv(self.w_precision)\n",
    "\n",
    "    def predict(self, X:np.ndarray):\n",
    "        \"\"\"\n",
    "        return mean and standard deviation of predictive distribution\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, n_features) np.ndarray\n",
    "            independent variable\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N,) np.ndarray\n",
    "            mean of the predictive distribution\n",
    "        y_std : (N,) np.ndarray\n",
    "            standard deviation of the predictive distribution\n",
    "        \"\"\"\n",
    "        y = X @ self.w_mean\n",
    "        y_var = 1 / self.beta + np.sum(X @ self.w_cov * X, axis=1)\n",
    "        y_std = np.sqrt(y_var)\n",
    "        return y, y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Start by generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_toy_data(func, 50, 0.25)\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "y_test = func(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For poynomial orders $M=\\{5, 10, 25, 50, 75, 100\\}$, construct the design matrix for the training data and fit the data using the `BayesianRegression` class with hyperparamters $\\alpha=1\\times10^{-10}$ and $\\beta=2$. \n",
    "\n",
    "Then generate the design matrix for the test data and predict the target values for the test data. (_hint: we followed the same steps in HW1, but this time you need pass parameters `alpha` and `beta` to the [class constructor](https://www.geeksforgeeks.org/constructors-in-python/) of_ `BayesianRegression`). \n",
    "\n",
    "Finally, simiarly to HW1, using a plotting library of your choosing (e.g. matplotlib) make a graph showing:\n",
    "\n",
    "- the function $f(x)$ i.e. `(x_test, y_test)`, shown as a solid line\n",
    "- the training data `(x_train, y_train)`, shown as circles\n",
    "- the mean of the predictive distribution `(x_test, y_fit)`, shown as a solid line\n",
    "- the standard deviation around the mean (_hint: if you are using matplotlib, you can use the function_ [`plt.fill_between`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.fill_between.html))\n",
    "\n",
    "Add a legend to the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "Do the same as in question 2.1 using RidgeRegression (class definition is in HW1) and compare the results to the fits obtained by BayesianRegression. Comment on the performance of the two methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
