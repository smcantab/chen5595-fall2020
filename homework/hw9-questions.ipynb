{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW focuses on bias-variance decomposition. It consists only of coding exercises. I provide several definitions and hints throughout that will help you.  If you are confused about anything, do not panic, send us a message on Piazza! We are here to help you learn.\n",
    "\n",
    "Please answer the numerical exercises using an ipython notebook **in [Google Colab](https://colab.research.google.com/)** (provide the link with your handwritten homework). **Please answer each coding problem  in a different cell**.\n",
    "\n",
    "You can also view this ipython notebook in your browser through [binder]() or through nbviewer [link]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Start by importing the functions and classes you implemented in HW7, namely `func`, `create_toy_data`, `GaussianFeature`, `SigmoidalFeature`, `build_design_matrix`. Then implement a `PolynomialFeature` class to construct the polynomial feature vector $\\mathbf{\\phi}(x) = (1, x, x^2, \\dots, x^M)$ where $x$ is a 1-dimensional (scalar) input. The class should be of the form\n",
    "\n",
    "```python\n",
    "class PolynomialFeature(object):\n",
    "    def __init__(self, degree):\n",
    "        self.deg = degree\n",
    "       \n",
    "    def get_feature_vec(self, x):\n",
    "        # returns polynomial feature vector for 1d input\n",
    "```\n",
    "\n",
    "What is the main difference between Gaussian or sigmoidal features and polynomial features? \n",
    "\n",
    "Then write a class to perform regularized linear regression using a quadratic regularizer, also known as _ridge regression_, by noting that the exact minimizer for ridge regression is given by\n",
    "\n",
    "$$\\mathbf{w}_{ML} = (\\lambda\\mathbf{I} + \\mathbf{\\Phi}^\\top \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^\\top \\mathbf{t}$$\n",
    "\n",
    "The class should be of the form\n",
    "\n",
    "\n",
    "```python\n",
    "class RidgeRegression(object):\n",
    "    def __init__(self, regcoeff):\n",
    "        self.regcoeff = regcoeff\n",
    "    \n",
    "    def fit(self, W, t):\n",
    "        # W is the design matrix for the training data and t is the corresponding target variables\n",
    "        # compute the weights self.w\n",
    "    \n",
    "    def predict(self, W):\n",
    "        # W is the design matrix for the test data\n",
    "        # returns the prediction y(x, self.w)\n",
    "```\n",
    "\n",
    "With these functions at hand we are now ready to explore the bias-variance decomposition for a simple regression problem. Generate 100 datasets each containing $N=25$ data points $t$ defined as follows\n",
    "\n",
    "$$t = f(x) + \\epsilon$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$f(x) = \\sin(2 \\pi x) + \\sin(4 \\pi x)$$\n",
    "\n",
    "and\n",
    "    \n",
    "$$\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma)$$\n",
    "\n",
    "Use the functions `func` and `create_toy_data` to generate these datasets. The data sets $\\mathcal{D}^{(\\ell)}$ are indexed by $\\ell=1, \\dots, L$ where $L=100$. For each of the datasets fit a model with $M=24$ Gaussian basis functions (means equally spaced between 0 and 1, _Hint: use_ `np.linspace(0, 1, 24)`) with variance $s^2=0.1$ by utilizing ridge regression. For each value of the regularization coefficient $\\lambda=100, 10^{-2}, 10^{-9}$ plot two graphs (e.g. using matplotlib):\n",
    "\n",
    "- the first plot should show curves corresponding to fits for 20 individual datasets $D^{(\\ell)}$;\n",
    "- the second plot should show the average of the fits for all $L=100$ datasets, against the regression function $f(x)$ defined above.\n",
    "\n",
    "The final figure should be analogous to Fig. 3.5 of PRML. Then do the same for a linear model using a basis of $M=24$ sigmoidal features with $s=10$, as well as for polynomial features of degree $M=24$.\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "Now we wish to examine the bias-variance tradeoff quantitatively for this example. The average prediction is estimated from\n",
    "\n",
    "$$\\overline{y}(x) = \\frac{1}{L} \\sum_{\\ell=1}^L y^{(\\ell)}(x)$$\n",
    "\n",
    "and the _integrated squared bias_, and the _integrated variance_, are given by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "(\\mathrm{bias})^2 &= \\frac{1}{N} \\sum_{n=1}^N \\left\\{ \\overline{y}(x_n) - f(x_n)\\right\\}^2 \\\\\n",
    "\\mathrm{variance} &= \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{L} \\sum_{\\ell=1}^L \\left\\{ y^{(\\ell)}(x_n) - \\overline{y}(x_n)\\right\\}^2 \n",
    "\\end{aligned}$$\n",
    "\n",
    "Generate a plot containing $(\\mathrm{bias})^2$, $\\mathrm{variance}$, and $(\\mathrm{bias})^2+\\mathrm{variance}$ for several choices of the regularization coefficient $\\lambda$. Do this for Gaussian, sigmoidal and polynomial features. The final figures should be analogous to Fig. 3.6 of PRML. \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "Referring to the results from Problem 1 and 2, describe in your own words the notion of Bias-Variance decomposition in the context of linear basis function models for regression and provide a mathematical derivation. _Hint: follow the discussion in the lecture notes_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
