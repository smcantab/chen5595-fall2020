{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEN 5595 Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This HW focuses on probability distributions. It consists of both hand-written and coding exercises. I provide several definitions and hints throughout that will help you.  If you are confused about anything, do not panic, send us a message on Piazza! We are here to help you learn.\n",
    "\n",
    "Please answer all the questions involving proofs/demonstrations using pen and paper, and all the numerical exercises using an ipython notebook **in [Google Colab](https://colab.research.google.com/)** (provide the link with your handwritten homework). **Please answer each coding problem  in a different cell**.\n",
    "\n",
    "You can also view this ipython notebook in your browser through [binder]() or through nbviewer [link]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "The Bernoulli distribution for a random variable $x \\in \\{ 0, 1\\}$ is given by\n",
    "\n",
    "$$ \\mathrm{Bern}(x|\\mu) = \\mu^x (1-\\mu)^{1-x} \\label{eq:bern} \\tag{1}$$\n",
    "\n",
    "Verify that the Bernoulli distribution satisfies the following properties\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sum_{x=0}^{1} p (x|\\mu) &= 1 \\\\\n",
    "\\mathbb{E}[x] &= \\mu \\\\\n",
    "\\mathrm{var}[x] &= \\mu (1-\\mu)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Show that the entropy $H[x]$ of a Bernoulli distributed random binary variable $x$ is given by\n",
    "\n",
    "$$H[x] = -\\mu \\log \\mu - (1-\\mu) \\log (1-\\mu) $$\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "The form of the Bernoulli distribution given in Eq. \\eqref{eq:bern} is not symmetric between the two values of $x$. In some situations, it will be more convenient to use an equivalent formulation for which $x \\in \\{âˆ’1, 1\\}$, in which case the distribution can be written\n",
    "\n",
    "$$p(x|\\mu) = \\left ( \\frac{1-\\mu}{2} \\right)^{(1-x)/2} \\left ( \\frac{1+\\mu}{2} \\right)^{(1+x)/2}$$\n",
    "\n",
    "where $\\mu \\in [-1, 1]$. Show that this distribution is normalized, and evaluate its mean, variance and entropy. _Hint: note that the possible outcomes are_ $x=1$ _and_ $x=-1$. _Then follows the same steps required to complete Problem 1._ \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "The multivariate Gaussian distribution for a random variable $\\mathbf{x}$ is given by\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2 \\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\exp \\left \\{ -\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu})^\\top  \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right \\}$$\n",
    "\n",
    "where $\\mathbf{\\mu}$ is a $D$-dimensional mean vector, $\\mathbf{\\Sigma}$ is a $D \\times D$ covariance matrix, and $|\\mathbf{\\Sigma}|^{1/2}$ denotes the determinant of $\\mathbf{\\Sigma}$. Now consider a data set $\\mathbf{X} = (\\mathbf{x}_1, \\dots, \\mathbf{x}_N)^\\top$ in which the observations $\\{ \\mathbf{x}_n\\}$ are assumed to be i.i.d.. We can estimate the parameters of this distribution by maximum likelihood.\n",
    "\n",
    "**Q1**. Write down the log-likelihood function for the multivariate Gaussian.\n",
    "\n",
    "**Q2**. What are the sufficient statistics for the multivariate Gaussian distributions? Explain your reasoning.\n",
    "\n",
    "**Q3**. Taking the derivative of the log-likelihood function, derive the maximum likelihood estimator for the parameter $\\mu$, or in other words show that\n",
    "\n",
    "$$\\mu_{\\mathrm{ML}} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n$$\n",
    "\n",
    "**Q4**. To find the maximum likelihood solution for the covariance matrix of a multivariate Gaussian, we need to maximize the log likelihood function with respect to $\\mathbf{\\Sigma}$, noting that the covariance matrix must be symmetric and positive definite. Here we proceed by ignoring these constraints and doing a straightforward maximization. We start by noting that the part of the likelihood that depends on $\\mathbf{\\Sigma}$ can be rewritten using the \"trace trick\" as \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log p(\\mathbf{X}|\\mathbf{\\mu}, \\mathbf{\\Sigma}) &= - \\frac{N}{2} \\log |\\mathbf{\\Sigma}| - \\frac{1}{2}\\sum_{n=1}^{N} (\\mathbf{x} - \\mathbf{\\mu})^\\top  \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}) + \\mathrm{const.} \\\\\n",
    "&=  - \\frac{N}{2} \\log |\\mathbf{\\Sigma}| - \\frac{1}{2} \\sum_{n=1}^{N} \\mathrm{Tr}\\left [(\\mathbf{x} - \\mathbf{\\mu}) (\\mathbf{x} - \\mathbf{\\mu})^\\top \\mathbf{\\Sigma}^{-1} \\right]+ \\mathrm{const.}\n",
    "\\end{aligned} \\label{eq:loglsig} \\tag{2}$$\n",
    "\n",
    "We also note the following matrix calculus identities\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\log |\\Sigma| &= \\log |\\Sigma^{-1}| \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{A}} \\log |\\mathbf{A}| &= (\\mathbf{A}^{-1})^\\top \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{A}} \\mathrm{Tr}(\\mathbf{\\mathbf{x} \\mathbf{x}^\\top A}) &= (\\mathbf{x} \\mathbf{x}^\\top)^\\top\n",
    "\\end{aligned}$$\n",
    "\n",
    "Where the first identity uses the fact that the determinant of the inverse of a matrix is the inverse of the determinant. Now, using the first identity rewrite Eq. \\eqref{eq:loglsig} in terms of the precision $\\mathbf{\\Sigma}^{-1}$, then take the derivative of the log-likelihood with respect to the precision $\\mathbf{\\Sigma}^{-1}$ and set it to zero to find the maximum likelihood estimator for the covariance\n",
    "\n",
    "$$\\Sigma_{\\mathrm{ML}} = \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x} - \\mathbf{\\mu}_{\\mathrm{ML}}) (\\mathbf{x} - \\mathbf{\\mu}_{\\mathrm{ML}})^\\top$$\n",
    "\n",
    "Show all your work.\n",
    "\n",
    "**Q5**. Are the maximum likelihood estimators for the mean and covariance of the multivariate Gaussian biased? If so, show how to correct for the bias. _Hint: see HW2_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "The multivariate Gaussian distribution is constant on surfaces in $\\mathbf{x}$-space for which its quadratic form is constant or, in other words, for which the squared Mahalanobis distance is constant\n",
    "\n",
    "$$\\Delta^2 = (\\mathbf{x} - \\mathbf{\\mu})^\\top  \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})$$\n",
    "\n",
    "In this problem we will explore the geometrical form of the Gaussian distribution. Using `np.loadtxt` import the data file [`mvg_data.txt`](https://github.com/smcantab/chen5595-fall2020/blob/master/homework/mvg_data.txt) containing 5000 data points drawn from a 2-dimensional multivariate Gaussian with unknown mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$.\n",
    "\n",
    "**Q1**. Use the maximum likelihood estimators for the mean and the variance of the Gaussian derived in Problem 3 to estimate $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$ for the data provided.\n",
    "\n",
    "Because the matrix $\\mathbf{\\Sigma}$ is real and symmetric, its eigenvalues $\\{\\lambda_i\\}$ will be real and its eigenvectors $\\{\\mathbf{u}_i\\}$ can be chosen from an orthonormal basis, so that $\\mathbf{u}_i^\\top\\mathbf{u}_j = \\delta_{ij}$.\n",
    "\n",
    "**Q2**. Using the function `scipy.linalg.eigh` find the eigenvalue and eigenvectors for $\\mathbf{\\Sigma}$, then verify that the eigenvectors are indeed orthogonal.\n",
    "\n",
    "The Mahalanobis distance can thus be rewritten\n",
    "\n",
    "$$\\Delta^2 = \\sum_{i=1}^D \\frac{y_i^2}{\\lambda_i}$$\n",
    "\n",
    "where we have defined\n",
    "\n",
    "$$y_i = \\mathbf{u}_i^\\top(\\mathbf{x} - \\mathbf{\\mu})$$\n",
    "\n",
    "so we can interpret $\\{y_i\\}$ as a new coordinate system defined by the orthonormal vectors $\\mathbf{u}_i$ in which the squared Mahalanobis distance takes the standard form of the equation of an ellipse. \n",
    "\n",
    "**Q3**. Using a plotting library of your choosing (e.g. Matplotlib), generate a \"scatter plot\" for the data. Then draw the eigenvectors $\\{\\mathbf{u}_i\\}$ scaled (multiplied) by $\\sqrt{\\lambda_i}$. _Hint: make sure that the aspect ratio of your plot is set to 'equal' (e.g. in matplotlib you can use_ `ax.set_aspect('equal')`_)_\n",
    "\n",
    "**Q4**. Use the function `scipy.stats.multivariate_normal` to generate a countour plot of the probability density of the fitted Gaussian and overlay a scatter plot above the contour plot. _Hint: see the examples in the documentation for `scipy.stats.multivariate_normal`, [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html)_\n",
    "\n",
    "In order to project the data $\\mathbf{x}$ into the new coordinate system one can construct an orthogonal matrix $\\mathbf{U}$ whose rows are given by $\\mathbf{u}_i^\\top$, so that\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{U}(\\mathbf{x}-\\mathbf{\\mu})$$ \n",
    "\n",
    "**Q5**. Build the orthogonal matrix $\\mathbf{U}$ and use it to project the dataset $\\mathbf{X}$ into this new coordinate system. Compute the covariance matrix for the data in the new coordinate system and verify that it is diagonal. Then, using a plotting library of your choosing (e.g. Matplotlib), generate a \"scatter plot\" for the data in the new coordinate system, and verify that the Gaussian is indeed aligned with the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "The central limit theorem (CLT) tells us that the sum of a set of random variables, which is itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases. \n",
    "\n",
    "Let us consider the situation where we have $N$ random variables $x_1, \\dots, x_N$ each of which is sampled from the same distribution $p(x)$, and denote their mean as $z = (x_1 + \\dots + x_N)/N$. From the CLT we expect that $p(z)$ tends to a Gaussian as $N$ becomes large.\n",
    "\n",
    "**Q1**. Take $p(x)$ to be a **uniform distribution over the interval $[0, 1]$**. Let us defined a dataset $\\mathcal{D}_N=\\{z_1, z_2 \\dots, z_M\\}$ where $z_i = (x_1 + \\dots + x_N)/N$ and the random variables $x$ are drawn i.i.d. from $p(x)$. Using a plotting library of your choosing (e.g. Matplotlib) plot histograms of the random variables $z$ for $N=1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100, 1000$. _Hint: use_ `np.random.uniform`.\n",
    "\n",
    "**Q2**. Do the same as in Q1 but take $p(x)$ to be $Beta(x|a=0.3, b=0.1)$. _Hint: use_ `np.random.beta`.\n",
    "\n",
    "**Q3**. Do the same as in Q1 but take $p(x)$ to be $Poisson(x|\\lambda=10)$. _Hint: use_ `np.random.poisson`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
