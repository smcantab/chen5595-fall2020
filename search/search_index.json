{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About the course Overview This course is meant to be an introduction for advanced-undergraduates or graduate students to probabilistic machine learning (ML) and to recent advances at the intersection of quantum chemistry/materials science and ML. After an overview of the fundamental concepts necessary to tackle the subject, namely Probability Theory, Decision Theory, Information Theory, and an exploration of simple probability distributions, the students will be introduced to Mixture Models, Linear Methods, Kernel Methods and Neural Networks (NN). These topics will lay the foundation for a brief discussion of molecular descriptors, Behler-Parrinello NN, and possibly Message Passing NN and Kernel Methods for quantum chemistry. Practical Information Location : lectures will be held virtually via Zoom @Canvas Time : Tuesdays & Thursdays 11:15-12:30 am CDT Instructor : Stefano Martiniani (he/him) Instructor office hours : Thursday 1:30-2:30 pm CDT Teaching Assistant : Rishabh Gupta (he/him) TA office hours : Wednesday 1:30-2:30 pm CDT and Friday 10:00-12:00 am CDT Communication Class discussion : Please use Piazza ( sign-up link ) for anything technical e.g. homework or general project-related questions, or anything else that applies broadly to the class. I encourage you to post your questions on Piazza and to try answering your classmates' questions. You can do so anonymously if you do not feel comfortable asking or answering a question publicly. Assignments : will be posted on Canvas course page . See Homework for details. Email : contact instructor via email from (strictly) your UMN email account for any confidential issues. Please start the subject line with CHEN 5595. Pre-requisites You must be either a MatS/ChEn graduate student or an upper-division MatS/ChEn major who has taken ChEn 3201, MatS 3141. Students from other PhD programs/majors need to receive instructor's approval. You must have a comfortable knowledge of linear algebra and multivariate calculus. Familiarity with probabilities will be helpful but not strictly necessary. You must know at least one programming language, ideally Python. I will assume that you can transition to writing simple programs in Python autonomously (see Python Resources ).","title":"About"},{"location":"#about-the-course","text":"","title":"About the course"},{"location":"#overview","text":"This course is meant to be an introduction for advanced-undergraduates or graduate students to probabilistic machine learning (ML) and to recent advances at the intersection of quantum chemistry/materials science and ML. After an overview of the fundamental concepts necessary to tackle the subject, namely Probability Theory, Decision Theory, Information Theory, and an exploration of simple probability distributions, the students will be introduced to Mixture Models, Linear Methods, Kernel Methods and Neural Networks (NN). These topics will lay the foundation for a brief discussion of molecular descriptors, Behler-Parrinello NN, and possibly Message Passing NN and Kernel Methods for quantum chemistry.","title":"Overview"},{"location":"#practical-information","text":"Location : lectures will be held virtually via Zoom @Canvas Time : Tuesdays & Thursdays 11:15-12:30 am CDT Instructor : Stefano Martiniani (he/him) Instructor office hours : Thursday 1:30-2:30 pm CDT Teaching Assistant : Rishabh Gupta (he/him) TA office hours : Wednesday 1:30-2:30 pm CDT and Friday 10:00-12:00 am CDT","title":"Practical Information"},{"location":"#communication","text":"Class discussion : Please use Piazza ( sign-up link ) for anything technical e.g. homework or general project-related questions, or anything else that applies broadly to the class. I encourage you to post your questions on Piazza and to try answering your classmates' questions. You can do so anonymously if you do not feel comfortable asking or answering a question publicly. Assignments : will be posted on Canvas course page . See Homework for details. Email : contact instructor via email from (strictly) your UMN email account for any confidential issues. Please start the subject line with CHEN 5595.","title":"Communication"},{"location":"#pre-requisites","text":"You must be either a MatS/ChEn graduate student or an upper-division MatS/ChEn major who has taken ChEn 3201, MatS 3141. Students from other PhD programs/majors need to receive instructor's approval. You must have a comfortable knowledge of linear algebra and multivariate calculus. Familiarity with probabilities will be helpful but not strictly necessary. You must know at least one programming language, ideally Python. I will assume that you can transition to writing simple programs in Python autonomously (see Python Resources ).","title":"Pre-requisites"},{"location":"calendar/","text":"Course Calendar Lecture Date Topic Readings Project 1 9/8 Course intro PRML 1.1 Start assembling team 2 9/10 Polynomial regression, Intro to Probability Theory PRML 1.1-1.2 3 9/15 Probability densities, Moments, Linear Correlation coefficient, Bayesian Probability, Bayesian Parameter Estimation, Marginal Likelihood, Bayes Factors PRML 1.2.1-1.2.3 4 9/17 Gaussian distribution, Maximum Likelihood, Bayesian curve fitting, Model selection (cross validation) PRML 1.2.4-1.2.6 5 9/22 Curse of dimensionality, Decision Theory (classification) PRML 1.3-1.5.4 6 9/24 Decision Theory (regression), Intro to Information Theory PRML 1.5.4-1.6 7 9/29 Shannon Entropy, Huffman Coding, Physical Interpretation of Entropy, Lagrange Multipliers PRML 1.6, Appendix E Register team 8 10/1 Differential Entropy, Maximum Entropy Distributions, Conditional Entropy, Jensen's Inequality, KL Divergence PRML 1.6-1.6.1 9 10/6 Mutual Information, Intro to Density Estimation, Bernoulli / Binomial / Multinomial distribution PRML 1.6.1-2.2 10 10/8 Dirichlet distribution, Multivariate Gaussian (Geometric Interpretation), Partitioned Gaussians (Conditional distributions) PRML 2.2.1-2.3.1 11 10/13 Partitioned Gaussians (Conditional and Marginal distributions), Bayes' rule for Gaussian variables, Interpolation of noise-free data, Maximum Likelihood for Gaussians PRML 2.3.1-2.3.4, MLPP 4.3.2.2 1st TA sign-off deadline 12 10/15 Bayesian Inference for the Gaussian, Student's t-distribution, Periodic variables PRML 2.3.5-2.3.8 13 10/20 Von Mises distribution, Mixtures of Gaussians PRML 2.3.8-2.3.9, 9.2-9.2.2 Project proposal 14 10/22 Expectation Maximization for Gaussian Mixtures, relation to K-means clustering PRML 9.2.2, 9.3.2 10 min talk 15 10/27 K-means algorithm, K-medoids, Nonparametric Density Estimators, Histograms, Least-squares Cross Validation PRML 9.1, 2.5 16 10/29 Kernel Density Estimator, k-Nearest Neighbor methods (density estimation and classification), Linear Basis Function Models PRML 2.5.1-2.5.2, 3.1 17 11/3 Maximum Likelihood for Linear Basis Function Models, Design Matrix, Pseudoinverse, Singular Value Decomposition, Stochastic Gradient Descent, Lasso, Regression with Multiple Outputs PRML 3.1.1, 3.1.3-3.1.5 18 11/5 Bias-Variance Decomposition, Bayesian Linear Regression, Equivalent Kernel, Intro to Classification PRML 3.2-3.3.3, 4.0 19 11/10 Bayesian Linear Regression Predictive Distribution, Equivalent Kernel, Linear Models for Classification, Discriminant Functions, Least Squares for Classification PRML 4-4.1.3 20 11/12 Least Squares for Classification, Fisher's Linear Discriminant, Perceptron Algorithm, Intro to Probabilistic Generative Models, Logistic Sigmoid PRML 4.1.3, 4.1.7-4.2 21 11/17 Probabilistic Generative Models Maximum Likelihood solution, Probabilistic Discriminative Models, Logistic Regression PRML 4.2-4.2.2, 4.3-4.3.5, 4.4 22 11/19 Iterative Reweighted Least Squares, Laplace Approximation Progress report 23 11/24 Feed-Forward Neural Networks 24 12/1 Weight-space symmetries, Network Training. Guest talk by B. Cheng Title: ab initio thermodynamics with the help of machine learning 25 12/3 Gradient Descent Optimization, Error Backpropagation. Guest talk by J. Klicpera Title: Graph Neural Networks for Chemistry: The Balancing Act Between Physical Inductive Biases and Representational Expressiveness 2nd TA sign-off deadline 26 12/8 Regularization in Neural Networks, Mixture Density Networks. Guest talk by S. Schoenholz Title: JAX MD: A Framework for Differentiable Physics 27 12/10 Guest talk by P. Wirnsberger Title: Optimizing Free Energy Estimation with Machine Learning Final report 28 12/15 Project talks 20 min talk","title":"Calendar"},{"location":"calendar/#course-calendar","text":"Lecture Date Topic Readings Project 1 9/8 Course intro PRML 1.1 Start assembling team 2 9/10 Polynomial regression, Intro to Probability Theory PRML 1.1-1.2 3 9/15 Probability densities, Moments, Linear Correlation coefficient, Bayesian Probability, Bayesian Parameter Estimation, Marginal Likelihood, Bayes Factors PRML 1.2.1-1.2.3 4 9/17 Gaussian distribution, Maximum Likelihood, Bayesian curve fitting, Model selection (cross validation) PRML 1.2.4-1.2.6 5 9/22 Curse of dimensionality, Decision Theory (classification) PRML 1.3-1.5.4 6 9/24 Decision Theory (regression), Intro to Information Theory PRML 1.5.4-1.6 7 9/29 Shannon Entropy, Huffman Coding, Physical Interpretation of Entropy, Lagrange Multipliers PRML 1.6, Appendix E Register team 8 10/1 Differential Entropy, Maximum Entropy Distributions, Conditional Entropy, Jensen's Inequality, KL Divergence PRML 1.6-1.6.1 9 10/6 Mutual Information, Intro to Density Estimation, Bernoulli / Binomial / Multinomial distribution PRML 1.6.1-2.2 10 10/8 Dirichlet distribution, Multivariate Gaussian (Geometric Interpretation), Partitioned Gaussians (Conditional distributions) PRML 2.2.1-2.3.1 11 10/13 Partitioned Gaussians (Conditional and Marginal distributions), Bayes' rule for Gaussian variables, Interpolation of noise-free data, Maximum Likelihood for Gaussians PRML 2.3.1-2.3.4, MLPP 4.3.2.2 1st TA sign-off deadline 12 10/15 Bayesian Inference for the Gaussian, Student's t-distribution, Periodic variables PRML 2.3.5-2.3.8 13 10/20 Von Mises distribution, Mixtures of Gaussians PRML 2.3.8-2.3.9, 9.2-9.2.2 Project proposal 14 10/22 Expectation Maximization for Gaussian Mixtures, relation to K-means clustering PRML 9.2.2, 9.3.2 10 min talk 15 10/27 K-means algorithm, K-medoids, Nonparametric Density Estimators, Histograms, Least-squares Cross Validation PRML 9.1, 2.5 16 10/29 Kernel Density Estimator, k-Nearest Neighbor methods (density estimation and classification), Linear Basis Function Models PRML 2.5.1-2.5.2, 3.1 17 11/3 Maximum Likelihood for Linear Basis Function Models, Design Matrix, Pseudoinverse, Singular Value Decomposition, Stochastic Gradient Descent, Lasso, Regression with Multiple Outputs PRML 3.1.1, 3.1.3-3.1.5 18 11/5 Bias-Variance Decomposition, Bayesian Linear Regression, Equivalent Kernel, Intro to Classification PRML 3.2-3.3.3, 4.0 19 11/10 Bayesian Linear Regression Predictive Distribution, Equivalent Kernel, Linear Models for Classification, Discriminant Functions, Least Squares for Classification PRML 4-4.1.3 20 11/12 Least Squares for Classification, Fisher's Linear Discriminant, Perceptron Algorithm, Intro to Probabilistic Generative Models, Logistic Sigmoid PRML 4.1.3, 4.1.7-4.2 21 11/17 Probabilistic Generative Models Maximum Likelihood solution, Probabilistic Discriminative Models, Logistic Regression PRML 4.2-4.2.2, 4.3-4.3.5, 4.4 22 11/19 Iterative Reweighted Least Squares, Laplace Approximation Progress report 23 11/24 Feed-Forward Neural Networks 24 12/1 Weight-space symmetries, Network Training. Guest talk by B. Cheng Title: ab initio thermodynamics with the help of machine learning 25 12/3 Gradient Descent Optimization, Error Backpropagation. Guest talk by J. Klicpera Title: Graph Neural Networks for Chemistry: The Balancing Act Between Physical Inductive Biases and Representational Expressiveness 2nd TA sign-off deadline 26 12/8 Regularization in Neural Networks, Mixture Density Networks. Guest talk by S. Schoenholz Title: JAX MD: A Framework for Differentiable Physics 27 12/10 Guest talk by P. Wirnsberger Title: Optimizing Free Energy Estimation with Machine Learning Final report 28 12/15 Project talks 20 min talk","title":"Course Calendar"},{"location":"organization/","text":"Course Organization Lectures & teaching materials Lectures will be held virtually via Zoom @Canvas . They will be recorded an uploaded to YouTube on the same day to an unlisted playlist (link on Canvas course page ). Notes will be uploaded to the Canvas course page after lectures. Code examples will be uploaded to the course GitHub page . Books Main textbook : Pattern Recognition and Machine Learning by C. M. Bishop (free online version ). Official errata , more (unofficial) errata , algorithms in Python . Additional references : Dive into Deep Learning (free online book and video lectures ) Machine Learning meets Quantum Physics by K.T. Schutt et al. Data-driven Science and Engineering by S. L. Brunton and J. N. Kutz ( video lectures ) Homework There will be approx. 10 assignments (written and/or code). Assignments will be posted by the end of Friday and are due by the end of Friday (23:59) of the following week. Submit homework through Canvas course page . For code exercises you should provide a link to a Google Colab notebook with your code. The worst grade does not count (see scholastic dishonesty for exceptions), but you cannot skip any assignments (if you do it counts). You can be late by 2 days once , for whatever reason. Any time after that, late work will not be accepted. There can be additional accommodations in the case of properly documented family emergencies or illness. I encourage you to collaborate on homework, but please do so through Piazza to the extent that is possible. However, you must come up with your own solutions and write your own code. So, when answering questions on Piazza guide your classmates towards the solution, do not post fully worked solutions. Grading Policy Your grade will be based on these components: Homework : 50% Project : 50% (see Project Grading for the detailed breakdown) Scholastic dishonesty If we determine that there has been a clear instance of scholastic dishonesty, the work will be given a score of 0 (even if it's the project!) and it will count towards the final grade. In addition the case will be reported to OCS.","title":"Organization"},{"location":"organization/#course-organization","text":"","title":"Course Organization"},{"location":"organization/#lectures-teaching-materials","text":"Lectures will be held virtually via Zoom @Canvas . They will be recorded an uploaded to YouTube on the same day to an unlisted playlist (link on Canvas course page ). Notes will be uploaded to the Canvas course page after lectures. Code examples will be uploaded to the course GitHub page .","title":"Lectures &amp; teaching materials"},{"location":"organization/#books","text":"Main textbook : Pattern Recognition and Machine Learning by C. M. Bishop (free online version ). Official errata , more (unofficial) errata , algorithms in Python . Additional references : Dive into Deep Learning (free online book and video lectures ) Machine Learning meets Quantum Physics by K.T. Schutt et al. Data-driven Science and Engineering by S. L. Brunton and J. N. Kutz ( video lectures )","title":"Books"},{"location":"organization/#homework","text":"There will be approx. 10 assignments (written and/or code). Assignments will be posted by the end of Friday and are due by the end of Friday (23:59) of the following week. Submit homework through Canvas course page . For code exercises you should provide a link to a Google Colab notebook with your code. The worst grade does not count (see scholastic dishonesty for exceptions), but you cannot skip any assignments (if you do it counts). You can be late by 2 days once , for whatever reason. Any time after that, late work will not be accepted. There can be additional accommodations in the case of properly documented family emergencies or illness. I encourage you to collaborate on homework, but please do so through Piazza to the extent that is possible. However, you must come up with your own solutions and write your own code. So, when answering questions on Piazza guide your classmates towards the solution, do not post fully worked solutions.","title":"Homework"},{"location":"organization/#grading-policy","text":"Your grade will be based on these components: Homework : 50% Project : 50% (see Project Grading for the detailed breakdown)","title":"Grading Policy"},{"location":"organization/#scholastic-dishonesty","text":"If we determine that there has been a clear instance of scholastic dishonesty, the work will be given a score of 0 (even if it's the project!) and it will count towards the final grade. In addition the case will be reported to OCS.","title":"Scholastic dishonesty"},{"location":"project/","text":"Project The project is the focal point of this course. It will provide you with an opportunity to go through the full research cycle: You will have to assemble a team, identify an idea, write a proposal, conduct the research, write a report in the format of a research paper, and present it to your peers. The best possible outcome would be a publishable or close to publishable project. Start early , you have a lot to learn and last minute projects will likely do poorly. Timeline Register team (names) by Sept 29 (Tu week 4) Project proposal (1-2 pages) due Oct 20 (Tu week 7) Talk (10 minutes) on Oct 22 (Th week 7) Progress report (2-3 pages) on Nov 19 (Th week 11) Final report on Dec 10 (Th week 14) Talk (20 minutes) on Dec 15 (last day of instruction) The team You should identify class mates with common interest to form teams of 3-4 students. You can use Piazza to look for team mates. All team members will receive the same score, so make sure you are comfortable with your team. Research Idea The scope of the project is to produce original work at the interface of Machine Learning and Materials Science/Chemical Engineering. This encompasses an extremely broad spectrum of research topics, from synthetic chemistry to catalysis, to biomedical or biological engineering, to systems engineering, and much more. A good rule of thumb is to look at all the ( Core and Graduate ) faculty in CEMS: if you come up with an idea, and there is some faculty member working in that same general area, it is probably fair game. If you are unsure, ask us. Faculty I encourage you to consult with faculty with active research programs in an area that you may be interested in, as they may have data or ideas for possible research projects. This is nonexhaustive list of faculty that you might be interested in talking to (one or more of them, some are theorists and some are experimentalists) for some directions/data: Ellad Tadmor (AEM), Ryan Elliot (AEM), Qi Zhang (CEMS), Prodromos Daoutidis (CEMS), Ben Hackel (CEMS), Samira Azarin (CEMS), Wei-Shou Hu (CEMS), David Poerschke (CEMS), Casim Sarkar (BME), Jason Goodpaster (Chemistry), Ilja Siepmann (Chemistry). Reports All reports must be written in Latex using the NeurIPS style file . I suggest you use Overleaf for collaborative writing, it will also reduce the chance that you will run into problems with installations of Latex across platforms. The reports must present your work in a reproducible way. I encourage you to prioritize quality over quantity: a 20 pages report will not necessarily get a better score than a 6 pages report. You must reference the GitHub repository with the code that you used to produce your analysis. Talks Presentation will be during regular classes and should be made with slides via Zoom by one or more team members, it is up to you how you decide to allocate the time. A good heuristic is to have 1 or fewer slides per minute. Rehearse the talks with your team prior to the class presentations. Talks will be recorded unless you object to it. TA sign-off Your team must attend office hours to talk to the TA about your project at least once before the proposal and the final report is due. Overall these two meetings count 10% of the final project score. Grading The final project score breakdown goes as follows: Meeting deadline to assemble team : 10% First sign off from TA : 5% Proposal report (1-2 pages) : 5% Proposal talk (10 min) : 5% Progress report (2-3 pages) : 10% Second sign off from TA : 5% Final written report (6-20 pages) : 40% Final presentation (20 min) : 20% FAQ Is it sufficient to produce a thorough literature review as the project? No! A literature review is not a project. You must produce an original piece of research. For instance, you may apply a ML approach to solve a scientific or engineering problem, or you can devise/implement a ML tool that serves the needs of the community in that area. The project is your chance to do, experiment with new concepts and learn new tools.","title":"Project"},{"location":"project/#project","text":"The project is the focal point of this course. It will provide you with an opportunity to go through the full research cycle: You will have to assemble a team, identify an idea, write a proposal, conduct the research, write a report in the format of a research paper, and present it to your peers. The best possible outcome would be a publishable or close to publishable project. Start early , you have a lot to learn and last minute projects will likely do poorly.","title":"Project"},{"location":"project/#timeline","text":"Register team (names) by Sept 29 (Tu week 4) Project proposal (1-2 pages) due Oct 20 (Tu week 7) Talk (10 minutes) on Oct 22 (Th week 7) Progress report (2-3 pages) on Nov 19 (Th week 11) Final report on Dec 10 (Th week 14) Talk (20 minutes) on Dec 15 (last day of instruction)","title":"Timeline"},{"location":"project/#the-team","text":"You should identify class mates with common interest to form teams of 3-4 students. You can use Piazza to look for team mates. All team members will receive the same score, so make sure you are comfortable with your team.","title":"The team"},{"location":"project/#research-idea","text":"The scope of the project is to produce original work at the interface of Machine Learning and Materials Science/Chemical Engineering. This encompasses an extremely broad spectrum of research topics, from synthetic chemistry to catalysis, to biomedical or biological engineering, to systems engineering, and much more. A good rule of thumb is to look at all the ( Core and Graduate ) faculty in CEMS: if you come up with an idea, and there is some faculty member working in that same general area, it is probably fair game. If you are unsure, ask us.","title":"Research Idea"},{"location":"project/#faculty","text":"I encourage you to consult with faculty with active research programs in an area that you may be interested in, as they may have data or ideas for possible research projects. This is nonexhaustive list of faculty that you might be interested in talking to (one or more of them, some are theorists and some are experimentalists) for some directions/data: Ellad Tadmor (AEM), Ryan Elliot (AEM), Qi Zhang (CEMS), Prodromos Daoutidis (CEMS), Ben Hackel (CEMS), Samira Azarin (CEMS), Wei-Shou Hu (CEMS), David Poerschke (CEMS), Casim Sarkar (BME), Jason Goodpaster (Chemistry), Ilja Siepmann (Chemistry).","title":"Faculty"},{"location":"project/#reports","text":"All reports must be written in Latex using the NeurIPS style file . I suggest you use Overleaf for collaborative writing, it will also reduce the chance that you will run into problems with installations of Latex across platforms. The reports must present your work in a reproducible way. I encourage you to prioritize quality over quantity: a 20 pages report will not necessarily get a better score than a 6 pages report. You must reference the GitHub repository with the code that you used to produce your analysis.","title":"Reports"},{"location":"project/#talks","text":"Presentation will be during regular classes and should be made with slides via Zoom by one or more team members, it is up to you how you decide to allocate the time. A good heuristic is to have 1 or fewer slides per minute. Rehearse the talks with your team prior to the class presentations. Talks will be recorded unless you object to it.","title":"Talks"},{"location":"project/#ta-sign-off","text":"Your team must attend office hours to talk to the TA about your project at least once before the proposal and the final report is due. Overall these two meetings count 10% of the final project score.","title":"TA sign-off"},{"location":"project/#grading","text":"The final project score breakdown goes as follows: Meeting deadline to assemble team : 10% First sign off from TA : 5% Proposal report (1-2 pages) : 5% Proposal talk (10 min) : 5% Progress report (2-3 pages) : 10% Second sign off from TA : 5% Final written report (6-20 pages) : 40% Final presentation (20 min) : 20%","title":"Grading"},{"location":"project/#faq","text":"Is it sufficient to produce a thorough literature review as the project? No! A literature review is not a project. You must produce an original piece of research. For instance, you may apply a ML approach to solve a scientific or engineering problem, or you can devise/implement a ML tool that serves the needs of the community in that area. The project is your chance to do, experiment with new concepts and learn new tools.","title":"FAQ"},{"location":"resources/","text":"Hello world Python There are many great resources for learning Python. Here are some of my and my students' favourites. Conda/Anaconda/pip For a local installation of Python with many of the Data Science libraries you may want to use, I recommend installing Conda/Anaconda. Anaconda Installers Documentation Available ML packages conda is a package installer for Anaconda. pip is the Python package installer, it works similarly to conda. Integrated Development Environment (IDE) There are many great IDEs to write Python code, I list below some of the most popular and easier to use. Spyder , ships with Anaconda . It was developed specifically for scientific computing and it has a Matlab-like feel. Unless you already use another IDE, or have a strong preference, go with this. PyCharm . You can get the professional edition for free registering with your UMN account. Atom is what I use. It is highly customizable, but needs some work to be set up as a Python IDE, e.g. see this blog . Sublime Text is similar to Atom, to set it up as a Python IDE see for instance this blog . Guides/Tutorials Matlab vs. Python Real Python Tutorials Python like you mean it Python for beginners Official Python 3 Tutorial Geeks for Geeks getting started with Python Problem Solving with Algorithms and Data Structures using Python . This is a great online book. It teaches data structures and algorithms while also teaching Python in quite some depth. It\u2019s really easy to read and it will teach you all the basics to become a good (Python) programmer. Moving to Python from other languages IPython/Jupyter notebooks Jupyter / Jupyter Lab are shipped with Anaconda nbviewer is another way of sharing Jupyter notebooks Google Colaboratory (Colab) enables collaborative editing of ipython notebooks and has its own computational resource (even GPUs! How to enable GPUs and other Colab tutorials ) Binder is a tool to turn your GitHub repository in a \"binder\" of iPython notebooks. It is hosted on JupyterHub, but no easy/free GPUs I'm afraid! MSI Jupyter Notebooks service . If you need more computational resources I suggest you use the MSI Jupyter Notebook service. I am negotiating some compute time for you to use on MSI as part of this course. Alternatively, you can register as part of a PI's research group, if they are willing to lend you some of their MSI compute time. Libraries This is a nonexhaustive list of some of the better known Python libraries in this area, or less known but that I know of and like. Many of these libraries are shipped with the Anaconda installation. Scientific Computing NumPy Geeks for Geeks Numpy in Python SciPy Geeks for Geeks Data Analysis with Scipy ML & Data Science Pandas scikit-learn Choosing the right scikit-learn estimator PyTorch Tutorials (60 minutes blitz) Tensorflow Tutorials Keras Sonnet MXNet Tutorials Gluon (60 minutes blitz) JAX Quickstart ONNX PyTorch Geometric (there are more) Deep Graph Library (DGL) ML for Chemistry and Materials DeepChem Book: Deep learning for the life sciences: applying deep learning to genomics, microscopy, drug discovery, and more Tutorials JAX M.D. Paper Quickstart ML4Chem DScribe KLIFF TorchANI Paper SchnetPack Paper DeepPMD Manual ASAP Paper AMP Paper Visualization Seaborn Example gallery Matplotlib Geeks for Geeks Introduction to Matplotlib Plotly Dash Dash gallery GitHub page Cheat Sheets Python for Data Science Cheat Sheet Pandas Cheat Sheet Matplotlib Cheat Sheet Conda Cheat Sheet Data Sets QM# and other data sets MoleculeNet AFLOW CMR OQMD QCArchive kaggle Predicting Molecular Properties Standards OPTIMADE MolSSI Git/GitHub Git is a version control system that enable the collaborative development/storage of Software. GitHub is the most popular code hosting platform for version control and collaboration. Git Installation GitHub guides Getting started YouTube channel Stack Overflow Stack Overflow is a great forum to find answers to your programming questions and ask new ones if nobody has aver asked the same question. Parsing Stack Overflow is one of the best ways of learning. If you have a programming question, before writing a post on Piazza or contacting the TA , I suggest you check Stack Overflow. Other UMN computing Resources UMN Virtual Online Linux Environment is a full CSE Labs Linux desktop delivered virtually through an application called FastX. This means you can do homework, programming and even use engineering software from your laptop. As you can connect to the internet, you can connect to a VOLE desktop. Tools for teams Slack is a communication platform for teams that integrates well with a large number of tools ( e.g. Google Drive/Calendar, Zoom, GitHub, Workstreams.ai, Slite, etc.) Workstreams.ai is a Kanban board-like service for task management Slite is an internal knowledge-base platform for teams. It has a Slack-like interface and it is great way or organizing/sharing notes/knowledge. Maths/Stats In addition to the textbooks listed under Organization/Books , you may find useful the following resources: The Matrix Cookbook is a quick reference guide for matrix identities, relations, etc. d2l.ai Maths for Deep Learning is an informal overview of some of the mathematics required for this course, most of which you know already or that we will cover in the first part of the course. A Statistical Learning/Pattern Recognition Glossary by Thomas Minka Datasets with Varied Appearance and Identical Statistics: The datasaurus dataset Paper Blog","title":"Resources"},{"location":"resources/#hello-world","text":"","title":"Hello world"},{"location":"resources/#python","text":"There are many great resources for learning Python. Here are some of my and my students' favourites.","title":"Python"},{"location":"resources/#condaanacondapip","text":"For a local installation of Python with many of the Data Science libraries you may want to use, I recommend installing Conda/Anaconda. Anaconda Installers Documentation Available ML packages conda is a package installer for Anaconda. pip is the Python package installer, it works similarly to conda.","title":"Conda/Anaconda/pip"},{"location":"resources/#integrated-development-environment-ide","text":"There are many great IDEs to write Python code, I list below some of the most popular and easier to use. Spyder , ships with Anaconda . It was developed specifically for scientific computing and it has a Matlab-like feel. Unless you already use another IDE, or have a strong preference, go with this. PyCharm . You can get the professional edition for free registering with your UMN account. Atom is what I use. It is highly customizable, but needs some work to be set up as a Python IDE, e.g. see this blog . Sublime Text is similar to Atom, to set it up as a Python IDE see for instance this blog .","title":"Integrated Development Environment (IDE)"},{"location":"resources/#guidestutorials","text":"Matlab vs. Python Real Python Tutorials Python like you mean it Python for beginners Official Python 3 Tutorial Geeks for Geeks getting started with Python Problem Solving with Algorithms and Data Structures using Python . This is a great online book. It teaches data structures and algorithms while also teaching Python in quite some depth. It\u2019s really easy to read and it will teach you all the basics to become a good (Python) programmer. Moving to Python from other languages","title":"Guides/Tutorials"},{"location":"resources/#ipythonjupyter-notebooks","text":"Jupyter / Jupyter Lab are shipped with Anaconda nbviewer is another way of sharing Jupyter notebooks Google Colaboratory (Colab) enables collaborative editing of ipython notebooks and has its own computational resource (even GPUs! How to enable GPUs and other Colab tutorials ) Binder is a tool to turn your GitHub repository in a \"binder\" of iPython notebooks. It is hosted on JupyterHub, but no easy/free GPUs I'm afraid! MSI Jupyter Notebooks service . If you need more computational resources I suggest you use the MSI Jupyter Notebook service. I am negotiating some compute time for you to use on MSI as part of this course. Alternatively, you can register as part of a PI's research group, if they are willing to lend you some of their MSI compute time.","title":"IPython/Jupyter notebooks"},{"location":"resources/#libraries","text":"This is a nonexhaustive list of some of the better known Python libraries in this area, or less known but that I know of and like. Many of these libraries are shipped with the Anaconda installation.","title":"Libraries"},{"location":"resources/#scientific-computing","text":"NumPy Geeks for Geeks Numpy in Python SciPy Geeks for Geeks Data Analysis with Scipy","title":"Scientific Computing"},{"location":"resources/#ml-data-science","text":"Pandas scikit-learn Choosing the right scikit-learn estimator PyTorch Tutorials (60 minutes blitz) Tensorflow Tutorials Keras Sonnet MXNet Tutorials Gluon (60 minutes blitz) JAX Quickstart ONNX PyTorch Geometric (there are more) Deep Graph Library (DGL)","title":"ML &amp; Data Science"},{"location":"resources/#ml-for-chemistry-and-materials","text":"DeepChem Book: Deep learning for the life sciences: applying deep learning to genomics, microscopy, drug discovery, and more Tutorials JAX M.D. Paper Quickstart ML4Chem DScribe KLIFF TorchANI Paper SchnetPack Paper DeepPMD Manual ASAP Paper AMP Paper","title":"ML for Chemistry and Materials"},{"location":"resources/#visualization","text":"Seaborn Example gallery Matplotlib Geeks for Geeks Introduction to Matplotlib Plotly Dash Dash gallery GitHub page","title":"Visualization"},{"location":"resources/#cheat-sheets","text":"Python for Data Science Cheat Sheet Pandas Cheat Sheet Matplotlib Cheat Sheet Conda Cheat Sheet","title":"Cheat Sheets"},{"location":"resources/#data-sets","text":"QM# and other data sets MoleculeNet AFLOW CMR OQMD QCArchive kaggle Predicting Molecular Properties","title":"Data Sets"},{"location":"resources/#standards","text":"OPTIMADE MolSSI","title":"Standards"},{"location":"resources/#gitgithub","text":"Git is a version control system that enable the collaborative development/storage of Software. GitHub is the most popular code hosting platform for version control and collaboration. Git Installation GitHub guides Getting started YouTube channel","title":"Git/GitHub"},{"location":"resources/#stack-overflow","text":"Stack Overflow is a great forum to find answers to your programming questions and ask new ones if nobody has aver asked the same question. Parsing Stack Overflow is one of the best ways of learning. If you have a programming question, before writing a post on Piazza or contacting the TA , I suggest you check Stack Overflow.","title":"Stack Overflow"},{"location":"resources/#other-umn-computing-resources","text":"UMN Virtual Online Linux Environment is a full CSE Labs Linux desktop delivered virtually through an application called FastX. This means you can do homework, programming and even use engineering software from your laptop. As you can connect to the internet, you can connect to a VOLE desktop.","title":"Other UMN computing Resources"},{"location":"resources/#tools-for-teams","text":"Slack is a communication platform for teams that integrates well with a large number of tools ( e.g. Google Drive/Calendar, Zoom, GitHub, Workstreams.ai, Slite, etc.) Workstreams.ai is a Kanban board-like service for task management Slite is an internal knowledge-base platform for teams. It has a Slack-like interface and it is great way or organizing/sharing notes/knowledge.","title":"Tools for teams"},{"location":"resources/#mathsstats","text":"In addition to the textbooks listed under Organization/Books , you may find useful the following resources: The Matrix Cookbook is a quick reference guide for matrix identities, relations, etc. d2l.ai Maths for Deep Learning is an informal overview of some of the mathematics required for this course, most of which you know already or that we will cover in the first part of the course. A Statistical Learning/Pattern Recognition Glossary by Thomas Minka Datasets with Varied Appearance and Identical Statistics: The datasaurus dataset Paper Blog","title":"Maths/Stats"}]}